{"cells":[{"cell_type":"markdown","metadata":{"id":"HfmtORdYMUTo"},"source":["# 2025年 世界モデル コンペティション1 ベースライン\n","**最低限の要件**  \n","- 行動を出力する agent は引数として obs (numpy.ndarray) を受け取り，action (float) と prior を用いた観測の再構成画像 (numpy.ndarray) を返すように実装する．\n","- 評価時に行動を出力する際には `agent(obs)` と実装している．他の引数を定義しても問題ないが，評価時には他の引数を指定できないため，デフォルトの設定で評価モードにしておく必要がある．\n","\n","**書き換え可能箇所**  \n","- 準備: 必要なライブラリの追加．ただし提出ファイルの作成に google drive を参照するためマウントは削除しないでください．\n","- モデルの実装・学習: 利用したいアルゴリズム及びモデルに自由に変更して構いません．ただしエージェントの入出力の形式は要件を満たしてください．\n","\n","**書き換えてはいけない箇所**  \n","- 環境の設定: omnicampus 上の採点で利用する環境のため，修正しないでください．\n","- 補助機能の実装: エージェントが要件を満たしているか検証する関数になっています．\n","\n","\n","## 目次\n","### モデルの学習と保存\n","1. [準備]()\n","2. [環境の設定]()\n","3. [補助機能の実装]()\n","4. [モデルの実装]()\n","5. [モデルの学習]()\n","6. [エージェントの保存]()\n","7. [student_code.py の作成]()\n","\n","### 提出物の作成\n","8. [submission_tool の準備]()\n","9. [提出物の作成]()\n","10. [提出内容のプレビュー]()"]},{"cell_type":"markdown","metadata":{"id":"yDN8Ohc0Mh8C"},"source":["## 1. 準備"]},{"cell_type":"markdown","metadata":{"id":"LXOA_CGxNmTe"},"source":["必要なライブラリのインストール．各自必要なライブラリがある場合は追加でインストールしてください．  "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FZaErbWsMOOB","outputId":"8832c08a-8053-4f53-9266-d01708f26626"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting metaworld\n","  Downloading metaworld-3.0.0-py3-none-any.whl.metadata (9.7 kB)\n","Collecting pyvirtualdisplay\n","  Downloading PyVirtualDisplay-3.0-py3-none-any.whl.metadata (943 bytes)\n","Requirement already satisfied: gymnasium>=1.1 in /usr/local/lib/python3.11/dist-packages (from metaworld) (1.2.0)\n","Collecting mujoco>=3.0.0 (from metaworld)\n","  Downloading mujoco-3.3.7-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (41 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.11/dist-packages (from metaworld) (2.0.2)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from metaworld) (1.15.3)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (from metaworld) (2.37.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.1->metaworld) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.1->metaworld) (4.14.1)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.1->metaworld) (0.0.4)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mujoco>=3.0.0->metaworld) (1.4.0)\n","Requirement already satisfied: etils[epath] in /usr/local/lib/python3.11/dist-packages (from mujoco>=3.0.0->metaworld) (1.12.2)\n","Collecting glfw (from mujoco>=3.0.0->metaworld)\n","  Downloading glfw-2.10.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n","Requirement already satisfied: pyopengl in /usr/local/lib/python3.11/dist-packages (from mujoco>=3.0.0->metaworld) (3.1.9)\n","Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio->metaworld) (11.2.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=3.0.0->metaworld) (2025.3.2)\n","Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=3.0.0->metaworld) (6.5.2)\n","Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=3.0.0->metaworld) (3.23.0)\n","Downloading metaworld-3.0.0-py3-none-any.whl (36.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.7/36.7 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n","Downloading mujoco-3.3.7-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading glfw-2.10.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl (243 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.5/243.5 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyvirtualdisplay, glfw, mujoco, metaworld\n","Successfully installed glfw-2.10.0 metaworld-3.0.0 mujoco-3.3.7 pyvirtualdisplay-3.0\n"]}],"source":["# ライブラリインストール\n","!pip install metaworld pyvirtualdisplay"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"80DvrvXsPknl"},"outputs":[],"source":["# colab / ubuntu用\n","# 仮想ディスプレイの設定（レンダリング用）\n","!apt install -y xvfb > /dev/null 2>&1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VhXGgnTKPsZK","outputId":"fc1431ba-aaf0-4ca7-f529-4514cf173331"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyvirtualdisplay.display.Display at 0x7ab152025690>"]},"metadata":{},"execution_count":3}],"source":["from pyvirtualdisplay import Display\n","virtual_display = Display(visible=0, size=(400, 300))\n","virtual_display.start()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"72zXgw6dMZ3_"},"outputs":[],"source":["import time\n","import os\n","import gc\n","import random\n","from typing import Any, List, Tuple\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import gym\n","from gym.wrappers import ResizeObservation\n","import metaworld\n","from metaworld.env_dict import (\n","    ALL_V3_ENVIRONMENTS_GOAL_OBSERVABLE,\n","    ALL_V3_ENVIRONMENTS_GOAL_HIDDEN,\n",")\n","import torch\n","import torch.distributions as td\n","from torch.distributions import Normal, OneHotCategoricalStraightThrough\n","from torch.distributions.kl import kl_divergence\n","from torch import nn\n","from torch.nn import functional as F\n","from torch.nn.utils import clip_grad_norm_"]},{"cell_type":"markdown","metadata":{"id":"_eayBK1HPV5H"},"source":["## 2. 環境の設定  \n","**環境の設定については修正しないでください**  \n","- こちらで実装している環境を用いてOmnicampus上では評価を行います．  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pj-DQRVWCJwe"},"outputs":[],"source":["class GymWrapperMetaWorld(object):\n","    \"\"\"\n","    MetaWorld環境のためのラッパー\n","    \"\"\"\n","\n","    metadata = {\"render.modes\": [\"human\", \"rgb_array\"]}\n","    reward_range = (-np.inf, np.inf)\n","\n","    # __init__でカメラ位置に関するパラメータ（ cam_dist:カメラ距離，cam_yaw：カメラの水平面での回転，cam_pitch:カメラの縦方向での回転）を受け取り，カメラの位置を調整できるようにします.\n","    # 　同時に画像の大きさも変更できるようにします\n","    def __init__(\n","        self,\n","        name,\n","        seed=None,\n","        size=(64, 64)\n","    ) -> None:\n","        # os.environ[\"MUJOCO_GL\"] = \"egl\"\n","        os.environ[\"MUJOCO_GL\"] = \"glfw\"\n","\n","        task = f\"{name}-v3-goal-observable\"\n","        env_cls = ALL_V3_ENVIRONMENTS_GOAL_OBSERVABLE[task]\n","        self._env = env_cls(seed=seed, render_mode=\"rgb_array\")\n","        self._env.mujoco_renderer.camera_id = 1  # corner に相当\n","        self._env.mujoco_renderer.camera_name = None\n","        self._size = size\n","\n","    def __getattr(self, name: str) -> Any:\n","        return getattr(self._env, name)\n","\n","    @property\n","    def observation_space(self) -> gym.spaces.Box:\n","        width, height = self._size\n","        return gym.spaces.Box(0, 255, (height, width, 3), dtype=np.uint8)\n","\n","    @property\n","    def action_space(self) -> gym.spaces.Box:\n","        return self._env.action_space\n","\n","    # 　元の観測（低次元の状態）は今回は捨てて，env.render()で取得した画像を観測とします.\n","    #  画像，報酬，終了シグナルが得られます.\n","    def step(self, action: np.ndarray) -> (np.ndarray, float, bool, dict):\n","        _, reward, done, truncated, info = self._env.step(action)\n","        obs = self._env.mujoco_renderer.render(render_mode=\"rgb_array\")\n","        obs = np.flipud(obs)\n","        return obs, reward, done, truncated, info\n","\n","    def reset(self) -> np.ndarray:\n","        self._env.reset()\n","        obs = self._env.mujoco_renderer.render(render_mode=\"rgb_array\")\n","        obs = np.flipud(obs)\n","        return obs\n","\n","    def close(self) -> None:\n","        self._env.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KWcf0WHzPvw-"},"outputs":[],"source":["class RepeatAction(gym.Wrapper):\n","    \"\"\"\n","    同じ行動を指定された回数自動的に繰り返すラッパー. 観測は最後の行動に対応するものになる\n","    \"\"\"\n","    def __init__(self, env, skip=4, max_steps=100_000):\n","        gym.Wrapper.__init__(self, env)\n","        self.max_steps = max_steps if max_steps else float(\"inf\")  # イテレーションの制限\n","        self.steps = 0  # イテレーション回数のカウント\n","        self.height = env.observation_space.shape[0]\n","        self.width = env.observation_space.shape[1]\n","        self._skip = skip\n","\n","    def reset(self):\n","        obs = self.env.reset()\n","        return obs\n","\n","    def step(self, action):\n","        if self.steps >= self.max_steps:\n","            print(\"Reached max iterations.\")\n","            return None\n","\n","        total_reward = 0.0\n","        self.steps += 1\n","        for _ in range(self._skip):\n","            obs, reward, done, truncated, info = self.env.step(action)\n","\n","            total_reward += reward\n","            if self.steps >= self.max_steps:\n","                done = True\n","\n","            if done or truncated:\n","                break\n","\n","        return obs, total_reward, done, truncated, info"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q18Zg5DmSKhe"},"outputs":[],"source":["def make_env(env, seed=None, img_size=64, max_steps=None):\n","    # シード固定\n","    env.action_space.seed(seed)\n","    env.observation_space.seed(seed)\n","    env = ResizeObservation(env, (img_size, img_size))\n","    env = RepeatAction(env=env, skip=1, max_steps=max_steps)\n","\n","    return env"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vp4v1_DpCMGQ","outputId":"815f456b-8526-4450-dd9e-526fd76079af"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]}],"source":["env = GymWrapperMetaWorld(\"hammer\", seed=0, size=(64, 64))\n","env = make_env(env, seed=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":104},"id":"oc3j4akPChtQ","outputId":"072b53fd-7733-45f2-d356-efdaf42cbf8d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[127, 126, 122],\n","        [127, 126, 122],\n","        [127, 126, 122],\n","        ...,\n","        [127, 126, 122],\n","        [127, 126, 122],\n","        [127, 126, 122]],\n","\n","       [[127, 126, 122],\n","        [127, 126, 122],\n","        [127, 126, 122],\n","        ...,\n","        [127, 126, 122],\n","        [127, 126, 122],\n","        [127, 126, 122]],\n","\n","       [[127, 126, 122],\n","        [127, 126, 122],\n","        [127, 126, 122],\n","        ...,\n","        [127, 126, 122],\n","        [127, 126, 122],\n","        [127, 126, 122]],\n","\n","       ...,\n","\n","       [[222, 223, 223],\n","        [215, 215, 215],\n","        [219, 219, 219],\n","        ...,\n","        [224, 224, 224],\n","        [221, 222, 222],\n","        [217, 218, 218]],\n","\n","       [[213, 213, 213],\n","        [218, 219, 219],\n","        [222, 222, 222],\n","        ...,\n","        [224, 224, 224],\n","        [222, 223, 222],\n","        [219, 220, 220]],\n","\n","       [[223, 223, 223],\n","        [225, 225, 225],\n","        [223, 224, 224],\n","        ...,\n","        [226, 226, 226],\n","        [226, 226, 226],\n","        [222, 223, 223]]], dtype=uint8)"],"text/html":["<style>\n","      .ndarray_repr .ndarray_raw_data {\n","        display: none;\n","      }\n","      .ndarray_repr.show_array .ndarray_raw_data {\n","        display: block;\n","      }\n","      .ndarray_repr.show_array .ndarray_image_preview {\n","        display: none;\n","      }\n","      </style>\n","      <div id=\"id-a44c1631-b0c1-4212-bccb-24f31417a2c7\" class=\"ndarray_repr\"><pre>ndarray (64, 64, 3) <button style=\"padding: 0 2px;\">show data</button></pre><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAIAAAAlC+aJAAAV0klEQVR4nO16a4wl13FeVZ3Tfbv7PufO+7Ezsy/uLLl8LB9LcknTTCyKZGwDpkTDjhFBEiwBBAwFsGzYhBMof6U/RhxYARJJAQyEUWRZUWLDSSRZpCXS4mNJ7q615HK53NmZnffz3tu33+dR+dGzg4VoBSIiZBOA9WPQaHSj6ztV9VXVdwf/1Rf+CP5/NrrZDvyf2ocAbrZ9COBm24cAbrZ9COBm24cAbrZ9COBm24cAbrZ9COBm2/8bAJjFxqbY3gHmD/qq/Dm60Wy22u3Bzc31OI5/9rc4Tqo/eHFgaxsCPzo+1ztx6wf66M8BQKPZnJ6enZ092B4ctJYRIIqi1dXl1dWVzY2NPM/+dy+vrDZ+9GrAXGs0Ov1+vrwC/9cANBqN6ZmDBw8eajZbAAC4F39mDoLg6C1ztxw7bozudbvr62urqyubmxtG6/3XOUnk4rX2m+eH6nUj5bWtrZCtHmz7H9CNDwyg2WxNT89Mzx5sNlsIIIQABLaMiABgmQEYAJgtoRBCtgcH2+3BW289oY3Z2tzYWF9bXV4KX3m1cflKw2g6fKi7srqwvj6f54b5+C1HP6g/PyuARqMxM3PwwPTs4OAgElpjkVCQMMZobQAYEQlJCLKWAUEIYgajFAMjIBE5Uo5PTAwPDLhn3lh/b35yYEArdXFzczGK3ktTh+iI543OX90+eefPE0Cz2ZqZmZ2emW21Bqy1AAAIAIBEwKy0JkIpBJcGzJYRAQC01sAQLSzE1xaHT59GtwIA0fLKu3/6p2J5+cjY2NWlpfmtreGPfvSzn/vcf/jMZ0Y7nXHXlRfejoaHsqnJnx0A/oPCVqPRnJk9eGB6ZmCgLQQxM1smIhLCaK21BgBERCJCRMKydoUQzFyoAhiQMN7ZffGzn/WyrH769KkvfGH95ZevffWrw44TF8U7i4vdWu0X/uAPjj38MDAvXrjwnd/5nTGiGpHw/ewTv9UDzvMcAJj3kvNnikCz2ZqemZ2ZPTjQGjDGAAIRIiADI6FltkohoZACGJiZrWUiZEREYFZKIaIjHcvMbFfeemtpe3vM94tXX33l2Wera2vVIHhrYWE9jsefeOKXP/e5arNprbXWTh4/Pvn002vf+AY7TjPLhn/w0kf/zZ90+/0XvvQl/7UzphoUR4/Ed93xUwGUfDIzMzs0NMwAxhgGcFwXEbXWSitAZLYIiISlu5aZiISUzFYVyrIFBiEFAzAzIaZZcfHtt6/luRRiplrtXr3aSZKN+flVx/nIv/ijU48/UZ4uIJbPf/SZZ75y5szu1asCQF6+/O6//pNjv//7K0vLblEMM1cXlw5+4hPr66u7Ozt7mbyfQq+99mq7PYiECIiI1loGBgYighIM8959ZiGFIGGZS8rkPcIpo4zWGGYGgNdee+35559PFxacH//YF2KqWi3yPBQinpvbrVbjOB4fH7/r5Mm7T54cGR1xHIeQjDEb16595VOfGlTqoO97Uo4+/viPXn89X1ioEc099WsPfP73iEgptba2srKysrGxFvZ6zIzz8/NKKSISUiAg32AkCJGM0WUBlB7vM6axBhhIiJJ5rLWEeGV+/vnnv//OxXe01vUoGnz33YRZCPQOT7bvukcHrTQvVlZW8jwvigIApqam5ubmbr/99tnZWSHED7/1re996UsHKpUx1zVE2/U6b215AB/54z+eOHUf8B6FGGMEiSRJ1tdXJQkKnMBYU+TFfsWUf5kZgAmJBRMSEVlrtdVKKUAUgogIEIEBETfWN/7sz3qIf6l1Ya1FnVd0bI2pIdbvmatNjEC+o9MtIbzhqogq9VzpoiiSJDlz5sxLL70UBMHc3NyJEycaJ07oS5f6ShXM/ShqCuEEwfDtJ8qqAwbLbK0VJIIgmJ09JAHQWmuMLUmFgY02QggiKrmSAcrIaK3LnHFdFxCssUorQSJKkldeeeXcuQsvvnjX/fcbo7MBUYwM+8niwv8Mw5YQH7PAbIwx1midJMN+ECjVZ5u7FXQdx62VATl//vyZM2fQcW4lYoDMWokoEEfvusvxfCEFW1Zaa60c6VhrEVEIIa0xSEiEiNIaQ0jkkLXWsiULsNdfbVk6ruMysTbGGiuImPmNN984f+58mqYA+v77f+iadKqGqhL0+/03P/uZj33j67i8mVtwjVFFYRnq9UaSF0kSE8BITepiG8CvB43Iraa5Y4zBOK4gMkDB7CISwNDcHCBobRCACKWQQgh73SQgEFJJfCUOIQgB95MKAMqmy8zGGCISgqwxFy9ePHv2bBiGRVFkWaayZFRkIy1PqyJPMseRW/fd17p0ZvJjvypdeensayhEs17vhlE37JMQtWpVK2WNIbb1ivBVuhomXqiGV1cBwDIzooOIAN0XX+RPfxpxLwuEFAAACAwghJCEBACIgPsjDYNla4wFBEECELVSACiIkJABFhcW33zzzZ2dnTzP8zxP08Qr+jM1ksJJopBIBEFVKP34X387z4vm0NDW6nLF8yqer4pcq9x1K57vqTy3zCSEW/E7nY5RhW954NpyxRgD0LMWmSWRQEx2dxFAa80A1lqJjrUWEBzHYWslIgICWBAkGNlau9dooXxiL+kLVQBzd6d79uzZ1dXV8tSzLDNJOCqygQEvT9M0U0JKIRwAqAgc2bwWZVl04cecJVLKPE2iJKnXG4wUhn2tNQnpen4a97VSgJinOKmNJ4Qj5WKvNyqlRLQA408/jYIkoSqU4zjArI0GQCmBkOT1KgVjLQBYaxDJcZwyXtoYQhRCaKXPnz+/sLBw/dTTPE2CojtacxDcNOoDoJQOSWmNUUplP3jDOXms2qiu6dSi0aqI4mRgcNhxHFVkfsUxzJ5fTeNQFQUAYGuyu3qlKIqJdntgbOzsuXMSkQBO/u7vHn/qKWtt2ZqklMZaIkGI1lgmkEVRuK6rtLbGCCkAsWzAiEiCHMQsyy5cuHD58uU0TcuDT9MUk864LGotP4371loSgkgAolGq5OBePxlx3KBas1oVSqVp2h4eEUhFnhqtA98TlaC7u6NVwdY6g5ObqSHfS63FIDhy6FDl/HmB6Pj+1H33lS2zZLyybVsAy1YIQYIkICZp4jpupVIpWbKcfo21qiiuXLly6dKlJImzLC+KPE2zPA5bJmzXPWtk1OuQEEI6JIQ1ho1hZmarlDrw8Y+4rquVKopCKdUeGiEiVWRaaymlF9RUr2OMMsYEQxPbxi2Kft3lyJjt3d2VlZWa60oiWamoPDdl9hsrpaONobIlEVprjTZSCFFx3bKnuK5LghDRsr22uPj222/3+/08z7Msy/M8TWIn3Z2q2ErgZUmstRbSEY7DzEYrtgzAWhtmW6vXkcgYk6WpZTswPApslSqssULIih8kYTfsx1rpantEDE3H81fdImKyIXM/TecXF6tCEMDQ7bcPHjmijTFKCSnYWgBgBiq7LSADy9JdKSUJoVWB6Kxtrl24cKHkxzLdsywzcXcQonrdV0UehT0SwnErAMDGGGvKfqeVElJ6XpWtZWuzNAGkVnvEGm2N0UoJ6Xh+kPR7YRTHSeLVW9Wpuc2dXZP2ZdFfLmSLKDWmE4aOtZq5PjFhmaUQZfNCQq200UaDdqRDQiCz1OUgJIQgSpV+/vkXer2elDJN071iTaIg3x0MHCIvjfrWGikdJEIAay1zWfpstHI9v+L51mgGTKO+U/FrzZYucmOMMUZIxw9qcb8b9qM4TR0vaB++oxOlST88PDV67u1ezlwQ5dYWWruIxOyOjTGzNsZ1XCSy1pTcf+PQKR3HAQQikkKG/fDMmTPj4+P9fr/8aqUIR2URNKt5Eqd5po1xHEcIwcDGmPKkjbWWuVpvSCmNMcbaNI78oBY0GroorLXGaCLh+UES9fr9KMkyRjE6d3e/MEkcD/o4ecsdL7z+lusiW1swG2aL6BI99/Wvt+bnH3zwwVP33+8JQUgW2VqLVJaeY63Fq1evAoCQEoCLvPjiF7+4u7t76NAhY3S6fKnlO1XPtbqwxgCSkEJKaY3VWiGAZVZKMWC92XKk1FqpPC+KvFpvekEtzxK2VquChAhqjTSOwjAMoxiJxubuLmS10+l0V67sxqobJb1er21Me2NjXMpBITwiJHp9ZGTL9wGgVqudPn36kUcemZqauj6hMSJJIchYAwjGaGZ2Xff48eNjY2PLy8udTnd07m5bqW1sb/eiBIV0KxUppdbaGE1EDFAoJR2nNTAgELVWRZZmWVpvtb1qrchTYNaqICK/WsuSOIr6/TgBgOGDx9lvxXGcdTbWNrYsQBzHh6Po4M5OvRy9AAyzYe64brm1RFH03e9+98tf/jKUyx+AtWyN0VpLISRby8zWWGYerHmdZsPzvG63+/Y7705NTbWaw/HqlX6ae9o4AgGAEJVSSuugWvX9QCsFzHmeKaVagyPSdbM4YmvLrS2oNfIs7ffDXj+21g5OHfJGpjudbtzb5WgnVZaSfhzHBxCLSkXkeSAEEDEAI3oA/Rt24oceeggAgKEcHkgQWyYs13NEAPA87/DRozuX35QqHhkZGRoa2tzcXNzYrR68QzRG+kna7cfG2kIpY22j2fK9wGiDiEmSaGNag8NCiCKJmbmkpqDWUEUeR/1elFhrGkPjrdlboyiOwl5L6Md+47fHpqZjK+r1ethsHq7XK0QVgAqAiyilPNTrFcXeQCmEuOfee7XWlvcGaQQkIchYW6iCmaWUlnn6yLFf++Qz3aXL0fKlVqM2NDQkpbz07uXEaTQO3WUdf6sTRmlerdakENaaPEt3dnctc6PRQqQiT7XR1mhBVK03VJFH/bATRlopr9Ycu/XeKEmjKML+5j2PPv5Xf/3fFxavhWHIzJcqlYuOM1apDHseCOFJaaX0mKMoKtn82LFj4+NjAFwyabl1MbPUWiMSAGijiQUwzxw57gXVtLdTRGF98vBgu+267u7u7s4OT0+fcLrrMut1o7Qi84orozgOqtV6owkAOs8sszVaSsevNlSRRf2wHydFUXhBdebOh6KsSJIk31k9ed+D59965+LFiwCgtS576FnXXRocPKaUA+BZu2vMK47jACilHMc5deqUNVZKiYBIZLQuRxjpOI61plwXAZiI/CAYHptcS2NjVO/aO5XGUGNs1nGcKIquXLnSbreHxo7Y7eUo3O70isH2QKM5wGyLNFFaA7Pjun6toVWRJXEvTvIsd9zKzJ0P5kBpGvV3NmYPTFZaI9977ptlZpe6QTkFbzFvE3me53leHMdFURDRyMhImqan7j9VtmEklFJKIXRZxAhAJABBSIFIpbIwOjW9PP8uEQkSnoTulXPV8UOtVktK2e/3u93u1NQUuTWnt2bQCaO4ItAyM7NbqfhBzWgd93vdfpznBQkxc/t94DeTMAw7nZbUt9z78L/7918ph/ZS7CjzodQNhBDl+NRsNiuVilIqiqIHHngg8AMSBFxi1mU7cx1Hlos8Al7fyxgRJ2cPvfEiIqDW+s77f4Gcykv/49tUbdYGxqWUcRwvLCzU6/WxyeN5dy1Puj1VeK5TrwZBrWG0TvrdXpRkeUGI44eOe4OTvTAMw5Cijcd+85N/+Z3vb25u3nj8pcAHAOVFabu7u3uCsbWPPvooCQFQ7lqMgIgEwAB4XSwpXb/OSBPTh0hIRCLEnY3VqVtu+yefeGagUUuW33FY1ev1RqORpum7711JnKYdOGDQURZyFnmeR2G3F6dJmiPA4OTM4OHb4yTp9/vZ1rUHf/Ejb19ZPPP6G0KIEgBfP7Ib1ZD9++XWGwTB6OgoEZYKZ7nZaqWMscxM1zVOssz7cWy2h6q1ujaagbfWlplte2T047/9z//xr3ys2Fzk/latWq3X657nra6urm53xfhRqI8obVbXNzphlKQZIdTbIxO3nYqTOIqiaHvt+NEjTmvk2//1vwGAlHuSpuu6VA7H162EcaP8Zox59tlnv/bVr62vr5fPSCnLk2ZmUkUhiCwz4Z76UP5MNTZ9kEgAQ7/bydNMK83AD/zSk5/6/L8cqFbU5rwnoFar1Wo1rfWVK/NdLRJ/SIHI8xyYvWrt4N0PJ1kRx0nY2R728fgDj/75N/8iy7L9VCn9KwWSG++UN/evHcdRSr3wwgvP/uEfPvfcc0RUbmd7yjIDJGkCzMbYMkBlHY8fmDWWkdBoHXa2jbW9Xri7szs8ceDTv/eF+x76Rb29KLJuEPhBEPi+v7W1tbS2icOHqD3l+rXJ204lymZZFva61Fs//cRT3//bHy4vLe27VQZhX+i+EdWeVg9Q1nSZFOV9rfXewoW4hzzwfURK08ReZwNmzou8PTqOQjADEca9TlkpvV5va2sLkf7Rr/76P33m8zVhobPsSfJ9PwgCa+38/Hwuq61j9+boaq3DMMw2Fh9+7Mnlje2/e+kluMHKMtBavz9/yka7n2M3vnXnnXciYDnJI4BlS5bZdV3HrcRRVELP81wIOXvkmON65YQch122DIgMkKbp0tJSHMcTs0ee/GfPzBw6ip1lR0W+5wVB4Hne1tbW5feuFEXR7/ejzaVjtxxtjh741n/59k+qyqVWoPWN3u/nTLVa9X1fCLFfLSXmQ4cPK6WMNXsxsUxir4VBEARhP8yyNKgGFdcNqrWhsXFVFMzc2VoXQniehwBsrTFma2traWmJAU8/8dSTv/FJz6TUW61IKjE4jpPnebi9MRTIk4889p///Ju9Xg/eZ0KIUin7CQxlzpQwbgR27NixwXZbOrLIC6VUyb9U7iVl8LxKxVqrtbFsjTGjkwdIOq4js+62NWZsdLTRbDDzXl4iENHg4ODskWPTY0Nkcqe/XrGZ67rVajWJ+iLeeuSXP/7aG2cvvXv5/d7v53fp+n6i/0Q132gnTpwgIiLh+750nDzPC1WQNgYRS2Z13Eq1Vs/StCgKY8z4gVlENMZWQJ3/3l8k/d7AwMDY2Nj1jyEiLl9b+ME3v7azs33Hw48dODLn5qGX7RAbs7P0K0//lltrnjt3rvyx6B+0Wq12o/c3Xr/f7rjjDmYWRKVA5LiuIEFEVI4cFc8TgoQQ9Xo9z4ssy0anZhhJab281e3vbv3Nc/92c2mhWq1OTEy4rkuEqsjf+9F3NtZWJuZO3nrPA632oNFK6EJvXIE8njp6nBB7YZjneRRFxpj3+1Qy2I0VvF++P2GNRmNicrJkGm0MMAshLDOlSSKkvM5WqLU21jTqdUQM6k0/qCIwAK9sdXph/9W/+k9///LfOo4zOjpa9YPLL393Z33ZH5o8evdDvucJIgDWRklCy+z5gZSy1wsBwFobx3GWZfy+/4YoGcxxHMdxXNf9aflz2223SSHKXb6MEparfBAEZRs2WpdaAyAhoh/4qlDDE1OLYQcBJmYPh91ON1xbvvYfV+Yv/dKvf+rSy9/bWb5qKvW7H3myXqtaW8oAlggJwQ2qiOQHQRiG+04URaG19jzvRm4B2OtKPy1z9gEYYwQAI7K1QkoCAjD/C9hp1jpIPAF0AAAAAElFTkSuQmCC\" class=\"ndarray_image_preview\" /><pre class=\"ndarray_raw_data\">array([[[127, 126, 122],\n","        [127, 126, 122],\n","        [127, 126, 122],\n","        ...,\n","        [127, 126, 122],\n","        [127, 126, 122],\n","        [127, 126, 122]],\n","\n","       [[127, 126, 122],\n","        [127, 126, 122],\n","        [127, 126, 122],\n","        ...,\n","        [127, 126, 122],\n","        [127, 126, 122],\n","        [127, 126, 122]],\n","\n","       [[127, 126, 122],\n","        [127, 126, 122],\n","        [127, 126, 122],\n","        ...,\n","        [127, 126, 122],\n","        [127, 126, 122],\n","        [127, 126, 122]],\n","\n","       ...,\n","\n","       [[222, 223, 223],\n","        [215, 215, 215],\n","        [219, 219, 219],\n","        ...,\n","        [224, 224, 224],\n","        [221, 222, 222],\n","        [217, 218, 218]],\n","\n","       [[213, 213, 213],\n","        [218, 219, 219],\n","        [222, 222, 222],\n","        ...,\n","        [224, 224, 224],\n","        [222, 223, 222],\n","        [219, 220, 220]],\n","\n","       [[223, 223, 223],\n","        [225, 225, 225],\n","        [223, 224, 224],\n","        ...,\n","        [226, 226, 226],\n","        [226, 226, 226],\n","        [222, 223, 223]]], dtype=uint8)</pre></div><script>\n","      (() => {\n","      const titles = ['show data', 'hide data'];\n","      let index = 0\n","      document.querySelector('#id-a44c1631-b0c1-4212-bccb-24f31417a2c7 button').onclick = (e) => {\n","        document.querySelector('#id-a44c1631-b0c1-4212-bccb-24f31417a2c7').classList.toggle('show_array');\n","        index = (++index) % 2;\n","        document.querySelector('#id-a44c1631-b0c1-4212-bccb-24f31417a2c7 button').textContent = titles[index];\n","        e.preventDefault();\n","        e.stopPropagation();\n","      }\n","      })();\n","    </script>"]},"metadata":{},"execution_count":9}],"source":["obs = env.reset()\n","plt.imshow(obs)"]},{"cell_type":"markdown","metadata":{"id":"UzkkdGoQSP6Q"},"source":["## 3. 補助機能の実装  \n","- set_seed: torchのシード値を固定できる関数です．   \n","- check_output_type: エージェントが要件を満たしているか検証する関数です．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kwFmpA5qSb8I"},"outputs":[],"source":["def set_seed(seed: int) -> None:\n","    \"\"\"\n","    Pytorch, NumPyのシード値を固定します．これによりモデル学習の再現性を担保できます．\n","\n","    Parameters\n","    ----------\n","    seed : int\n","        シード値．\n","    \"\"\"\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u3SbQNwZMsQf"},"outputs":[],"source":["def check_output_type(agent, env):\n","    \"\"\"\n","    agent の入出力が要件を満たしているか検証する関数．\n","    agent は入力として環境からの観測をそのまま受取，環境に直接渡せる形式で出力を作成する必要がある．\n","    （テストケースも兼ねており，この関数で動作すれば採点環境でも動作する想定）\n","    \"\"\"\n","    action_dim = env.action_space.shape[0]\n","    obs = env.reset()\n","    action, pred_obs = agent(obs)\n","\n","    # int 型 + 行動次元に収まっているか確認\n","    assert action.shape[0] == action_dim and (np.abs(action) <= 1).all(), \"行動の出力形式を満たしていません．(shape (4, ), -1 <= action <= 1)\"\n","    assert pred_obs.shape[2] == 3 and (pred_obs >= 0).all() and (pred_obs <= 1).all(), \"観測の出力形式を満たしていません．(shape (64, 64, 3), 0 <= obs <= 1)\"\n","    _, reward, done, _, _ = env.step(action)\n","\n","    print(\"要件を満たしています．\")"]},{"cell_type":"markdown","metadata":{"id":"B3rWEGtkA3e_"},"source":["## 4. モデルの実装\n","**モデルの実装は他のアルゴリズム・モデルに変更していただいて構いません**  \n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","source":["class MSE(td.Normal):\n","    def __init__(self, loc, validate_args=None):\n","        super(MSE, self).__init__(loc, 1.0, validate_args=validate_args)\n","\n","    @property\n","    def mode(self):\n","        return self.mean\n","\n","    def sample(self, sample_shape=torch.Size()):\n","        return self.rsample(sample_shape)\n","\n","    def log_prob(self, value):\n","        if self._validate_args:\n","            self._validate_sample(value)\n","        # NOTE: dropped the constant term\n","        return -((value - self.loc) ** 2) / 2\n","\n","# From https://github.com/toshas/torch_truncnorm/blob/main/TruncatedNormal.py\n","import math\n","from numbers import Number\n","\n","import torch\n","from torch.distributions import Distribution, constraints\n","from torch.distributions.utils import broadcast_all\n","\n","CONST_SQRT_2 = math.sqrt(2)\n","CONST_INV_SQRT_2PI = 1 / math.sqrt(2 * math.pi)\n","CONST_INV_SQRT_2 = 1 / math.sqrt(2)\n","CONST_LOG_INV_SQRT_2PI = math.log(CONST_INV_SQRT_2PI)\n","CONST_LOG_SQRT_2PI_E = 0.5 * math.log(2 * math.pi * math.e)\n","\n","\n","class TruncatedStandardNormal(Distribution):\n","    \"\"\"\n","    Truncated Standard Normal distribution\n","    https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n","    \"\"\"\n","\n","    arg_constraints = {\n","        'a': constraints.real,\n","        'b': constraints.real,\n","    }\n","    has_rsample = True\n","\n","    def __init__(self, a, b, validate_args=None):\n","        self.a, self.b = broadcast_all(a, b)\n","        if isinstance(a, Number) and isinstance(b, Number):\n","            batch_shape = torch.Size()\n","        else:\n","            batch_shape = self.a.size()\n","        super(TruncatedStandardNormal, self).__init__(batch_shape, validate_args=validate_args)\n","        if self.a.dtype != self.b.dtype:\n","            raise ValueError('Truncation bounds types are different')\n","        if any((self.a >= self.b).view(-1, ).tolist()):\n","            raise ValueError('Incorrect truncation range')\n","        eps = torch.finfo(self.a.dtype).eps\n","        self._dtype_min_gt_0 = eps\n","        self._dtype_max_lt_1 = 1 - eps\n","        self._little_phi_a = self._little_phi(self.a)\n","        self._little_phi_b = self._little_phi(self.b)\n","        self._big_phi_a = self._big_phi(self.a)\n","        self._big_phi_b = self._big_phi(self.b)\n","        self._Z = (self._big_phi_b - self._big_phi_a).clamp_min(eps)\n","        self._log_Z = self._Z.log()\n","        little_phi_coeff_a = torch.nan_to_num(self.a, nan=math.nan)\n","        little_phi_coeff_b = torch.nan_to_num(self.b, nan=math.nan)\n","        self._lpbb_m_lpaa_d_Z = (self._little_phi_b * little_phi_coeff_b -\n","                                 self._little_phi_a * little_phi_coeff_a) / self._Z\n","        self._mean = -(self._little_phi_b - self._little_phi_a) / self._Z\n","        # NOTE: additional to github.com/toshas/torch_truncnorm\n","        self._mode = torch.clamp(torch.zeros_like(self.a), self.a, self.b)\n","        self._variance = 1 - self._lpbb_m_lpaa_d_Z - ((self._little_phi_b - self._little_phi_a) / self._Z) ** 2\n","        self._entropy = CONST_LOG_SQRT_2PI_E + self._log_Z - 0.5 * self._lpbb_m_lpaa_d_Z\n","\n","    @constraints.dependent_property\n","    def support(self):\n","        return constraints.interval(self.a, self.b)\n","\n","    @property\n","    def mean(self):\n","        return self._mean\n","\n","    @property\n","    def mode(self):\n","        return self._mode\n","\n","    @property\n","    def variance(self):\n","        return self._variance\n","\n","    def entropy(self):\n","        return self._entropy\n","\n","    @property\n","    def auc(self):\n","        return self._Z\n","\n","    @staticmethod\n","    def _little_phi(x):\n","        return (-(x ** 2) * 0.5).exp() * CONST_INV_SQRT_2PI\n","\n","    @staticmethod\n","    def _big_phi(x):\n","        return 0.5 * (1 + (x * CONST_INV_SQRT_2).erf())\n","\n","    @staticmethod\n","    def _inv_big_phi(x):\n","        return CONST_SQRT_2 * (2 * x - 1).erfinv()\n","\n","    def cdf(self, value):\n","        if self._validate_args:\n","            self._validate_sample(value)\n","        return ((self._big_phi(value) - self._big_phi_a) / self._Z).clamp(0, 1)\n","\n","    def icdf(self, value):\n","        return self._inv_big_phi(self._big_phi_a + value * self._Z)\n","\n","    def log_prob(self, value):\n","        if self._validate_args:\n","            self._validate_sample(value)\n","        return CONST_LOG_INV_SQRT_2PI - self._log_Z - (value ** 2) * 0.5\n","\n","    def rsample(self, sample_shape=torch.Size()):\n","        # icdf is numerically unstable; as a consequence, so is rsample.\n","        shape = self._extended_shape(sample_shape)\n","        p = torch.empty(shape, device=self.a.device).uniform_(self._dtype_min_gt_0, self._dtype_max_lt_1)\n","        return self.icdf(p)\n","\n","\n","class TruncatedNormal(TruncatedStandardNormal):\n","    \"\"\"\n","    Truncated Normal distribution\n","    https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n","    \"\"\"\n","\n","    has_rsample = True\n","\n","    def __init__(self, loc, scale, scalar_a, scalar_b, validate_args=None):\n","        self.loc, self.scale, a, b = broadcast_all(loc, scale, scalar_a, scalar_b)\n","        a = (a - self.loc) / self.scale\n","        b = (b - self.loc) / self.scale\n","        super(TruncatedNormal, self).__init__(a, b, validate_args=validate_args)\n","        self._log_scale = self.scale.log()\n","        self._mean = self._mean * self.scale + self.loc\n","        self._mode = torch.clamp(self.loc, scalar_a, scalar_b)  # NOTE: additional to github.com/toshas/torch_truncnorm\n","        self._variance = self._variance * self.scale ** 2\n","        self._entropy += self._log_scale\n","\n","    def _to_std_rv(self, value):\n","        return (value - self.loc) / self.scale\n","\n","    def _from_std_rv(self, value):\n","        return value * self.scale + self.loc\n","\n","    def cdf(self, value):\n","        return super(TruncatedNormal, self).cdf(self._to_std_rv(value))\n","\n","    def icdf(self, value):\n","        return self._from_std_rv(super(TruncatedNormal, self).icdf(value))\n","\n","    def log_prob(self, value):\n","        return super(TruncatedNormal, self).log_prob(self._to_std_rv(value)) - self._log_scale\n","\n","\n","class TruncNormalDist(TruncatedNormal):\n","\n","    def __init__(self, loc, scale, low, high, clip=1e-6, mult=1):\n","        super().__init__(loc, scale, low, high)\n","        self._clip = clip\n","        self._mult = mult\n","\n","        self.low = low\n","        self.high = high\n","\n","    def sample(self, *args, **kwargs):\n","        event = super().rsample(*args, **kwargs)\n","        if self._clip:\n","            clipped = torch.clamp(\n","                event, self.low + self._clip, self.high - self._clip\n","            )\n","            event = event - event.detach() + clipped.detach()\n","        if self._mult:\n","            event *= self._mult\n","        return event"],"metadata":{"id":"KudCq6cLeRzp"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bUt9Lp3HMeJq"},"outputs":[],"source":["class RSSM(nn.Module):\n","    def __init__(self, mlp_hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int, actino_dim: int):\n","        super().__init__()\n","\n","        self.rnn_hidden_dim = rnn_hidden_dim\n","        self.state_dim = state_dim\n","        self.num_classes = num_classes\n","\n","        # Recurrent model\n","        # h_t = f(h_t-1, z_t-1, a_t-1)\n","        self.transition_hidden = nn.Linear(state_dim * num_classes + action_dim, mlp_hidden_dim)\n","        self.transition = nn.GRUCell(mlp_hidden_dim, rnn_hidden_dim)\n","\n","        # transition predictor\n","        self.prior_hidden = nn.Linear(rnn_hidden_dim, mlp_hidden_dim)\n","        self.prior_logits = nn.Linear(mlp_hidden_dim, state_dim * num_classes)\n","\n","        # representation model\n","        self.posterior_hidden = nn.Linear(rnn_hidden_dim + 1536, mlp_hidden_dim)\n","        self.posterior_logits = nn.Linear(mlp_hidden_dim, state_dim * num_classes)\n","\n","    def recurrent(self, state: torch.Tensor, action: torch.Tensor, rnn_hidden: torch.Tensor):\n","        # recullent model: h_t = f(h_t-1, z_t-1, a_t-1)を計算する\n","        hidden = F.elu(self.transition_hidden(torch.cat([state, action], dim=1)))\n","        rnn_hidden = self.transition(hidden, rnn_hidden)\n","\n","        return rnn_hidden  # h_t\n","\n","    def get_prior(self, rnn_hidden: torch.Tensor, detach=False):\n","        # transition predictor: \\hat{z}_t ~ p(z\\hat{z}_t | h_t)\n","        hidden = F.elu(self.prior_hidden(rnn_hidden))\n","        logits = self.prior_logits(hidden)\n","        logits = logits.reshape(logits.shape[0], self.state_dim, self.num_classes)\n","\n","        prior_dist = td.Independent(OneHotCategoricalStraightThrough(logits=logits), 1)\n","        if detach:\n","            detach_prior = td.Independent(OneHotCategoricalStraightThrough(logits=logits.detach()), 1)\n","            return prior_dist, detach_prior  # p(z\\hat{z}_t | h_t)\n","        return prior_dist\n","\n","    def get_posterior(self, rnn_hidden: torch.Tensor, embedded_obs: torch.Tensor, detach=False):\n","        # representation predictor: z_t ~ q(z_t | h_t, o_t)\n","        hidden = F.elu(self.posterior_hidden(torch.cat([rnn_hidden, embedded_obs], dim=1)))\n","        logits = self.posterior_logits(hidden)\n","        logits = logits.reshape(logits.shape[0], self.state_dim, self.num_classes)\n","\n","        posterior_dist = td.Independent(OneHotCategoricalStraightThrough(logits=logits), 1)\n","        if detach:\n","            detach_posterior = td.Independent(OneHotCategoricalStraightThrough(logits=logits.detach()), 1)\n","            return posterior_dist, detach_posterior  # q(z_t | h_t, o_t)\n","        return posterior_dist"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VmsUzBFYMeJq"},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.conv1 = nn.Conv2d(3, 48, kernel_size=4, stride=2)\n","        self.conv2 = nn.Conv2d(48, 96, kernel_size=4, stride=2)\n","        self.conv3 = nn.Conv2d(96, 192, kernel_size=4, stride=2)\n","        self.conv4 = nn.Conv2d(192, 384, kernel_size=4, stride=2)\n","\n","    def forward(self, obs: torch.Tensor):\n","        \"\"\"\n","        観測画像をベクトルに埋め込むためのEncoder．\n","\n","        Parameters\n","        ----------\n","        obs : torch.Tensor (B, C, H, W)\n","            入力となる観測画像．\n","\n","        Returns\n","        -------\n","        embedded_obs : torch.Tensor (B, D)\n","            観測画像をベクトルに変換したもの．Dは入力画像の幅と高さに依存して変わる．\n","            入力が(B, 3, 64, 64)の場合，出力は(B, 1536)になる．\n","        \"\"\"\n","        hidden = F.elu(self.conv1(obs))\n","        hidden = F.elu(self.conv2(hidden))\n","        hidden = F.elu(self.conv3(hidden))\n","        embedded_obs = self.conv4(hidden).reshape(hidden.size(0), -1)\n","\n","        return embedded_obs  # x_t"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4c0l4yGSo9Fk"},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n","        super().__init__()\n","        self.fc = nn.Linear(state_dim*num_classes + rnn_hidden_dim, 1536)\n","        self.dc1 = nn.ConvTranspose2d(1536, 192, kernel_size=5, stride=2)\n","        self.dc2 = nn.ConvTranspose2d(192, 96, kernel_size=5, stride=2)\n","        self.dc3 = nn.ConvTranspose2d(96, 48, kernel_size=6, stride=2)\n","        self.dc4 = nn.ConvTranspose2d(48, 3, kernel_size=6, stride=2)\n","\n","\n","    def forward(self, state: torch.Tensor, rnn_hidden: torch.Tensor):\n","        \"\"\"\n","        決定論的状態と，確率的状態を入力として，観測画像を復元するDecoder．\n","        出力は多次元正規分布の平均値をとる．\n","\n","        Paremters\n","        ---------\n","        state : torch.Tensor (B, state_dim * num_classes)\n","            確率的状態．\n","        rnn_hidden : torch.Tensor (B, rnn_hidden_dim)\n","            決定論的状態．\n","\n","        Returns\n","        -------\n","        obs_dist : torch.distribution.Independent\n","            観測画像を再構成するための多次元正規分布．\n","        \"\"\"\n","        hidden = self.fc(torch.cat([state, rnn_hidden], dim=1))\n","        hidden = hidden.view(hidden.size(0), 1536, 1, 1)\n","        hidden = F.elu(self.dc1(hidden))\n","        hidden = F.elu(self.dc2(hidden))\n","        hidden = F.elu(self.dc3(hidden))\n","        mean = self.dc4(hidden)\n","\n","        obs_dist = td.Independent(MSE(mean), 3)\n","        return obs_dist  # p(\\hat{x}_t | h_t, z_t)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uJ5OpjS4deuO"},"outputs":[],"source":["class RewardModel(nn.Module):\n","    def __init__(self, hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n","        super().__init__()\n","        self.fc1 = nn.Linear(state_dim*num_classes + rnn_hidden_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc4 = nn.Linear(hidden_dim, 1)\n","\n","    def forward(self, state: torch.Tensor, rnn_hidden: torch.Tensor):\n","        \"\"\"\n","        決定論的状態と，確率的状態を入力として，報酬を予測するモデル．\n","        出力は正規分布の平均値をとる．\n","\n","        Paremters\n","        ---------\n","        state : torch.Tensor (B, state_dim * num_classes)\n","            確率的状態．\n","        rnn_hidden : torch.Tensor (B, rnn_hidden_dim)\n","            決定論的状態．\n","\n","        Returns\n","        -------\n","        reward_dist : torch.distribution.Independent\n","            報酬を予測するための正規分布．\n","        \"\"\"\n","        hidden = F.elu(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n","        hidden = F.elu(self.fc2(hidden))\n","        hidden = F.elu(self.fc3(hidden))\n","        mean = self.fc4(hidden)\n","\n","        reward_dist = td.Independent(MSE(mean),  1)\n","        return reward_dist  # p(\\hat{r}_t | h_t, z_t)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QzJB9ZiOxMnn"},"outputs":[],"source":["class DiscountModel(nn.Module):\n","    def __init__(self, hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n","        super().__init__()\n","        self.fc1 = nn.Linear(state_dim*num_classes + rnn_hidden_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc4 = nn.Linear(hidden_dim, 1)\n","\n","    def forward(self, state: torch.Tensor, rnn_hidden: torch.Tensor):\n","        \"\"\"\n","        決定論的状態と，確率的状態を入力として，現在の状態がエピソード終端かどうか判別するモデル．\n","        出力はベルヌーイ分布の平均値をとる．\n","\n","        Paremters\n","        ---------\n","        state : torch.Tensor (B, state_dim * num_classes)\n","            確率的状態．\n","        rnn_hidden : torch.Tensor (B, rnn_hidden_dim)\n","            決定論的状態．\n","\n","        Returns\n","        -------\n","        discount_dist : torch.distribution.Independent\n","            状態が終端かどうかを予測するためのベルヌーイ分布．\n","        \"\"\"\n","        hidden = F.elu(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n","        hidden = F.elu(self.fc2(hidden))\n","        hidden = F.elu(self.fc3(hidden))\n","        mean= self.fc4(hidden)\n","\n","        discount_dist = td.Independent(td.Bernoulli(logits=mean),  1)\n","        return discount_dist  # p(\\hat{\\gamma}_t | h_t, z_t)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EyvUF6YHdk4v"},"outputs":[],"source":["class Actor(nn.Module):\n","    def __init__(self, action_dim: int, hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n","        super().__init__()\n","\n","        self.fc1 = nn.Linear(state_dim * num_classes + rnn_hidden_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n","        self.mean = nn.Linear(hidden_dim, action_dim)\n","        self.std = nn.Linear(hidden_dim, action_dim)\n","        self.min_stddev = 0.1\n","        self.init_stddev = np.log(np.exp(5.0) - 1)\n","\n","    def forward(self, state: torch.tensor, rnn_hidden: torch.Tensor, eval: bool = False):\n","        \"\"\"\n","        確率的状態を入力として，criticで推定される価値が最大となる行動を出力する．\n","\n","        Parameters\n","        ----------\n","        state : torch.Tensor (B, state_dim * num_classes)\n","            確率的状態．\n","        rnn_hidden : torch.Tensor (B, rnn_hidden_dim)\n","            決定論的状態．\n","\n","        Returns\n","        -------\n","        action : torch.Tensor (B, 1)\n","            行動．\n","        action_log_prob : torch.Tensor(B, 1)\n","            予測した行動をとる確率の対数．\n","        action_entropy : torch.Tensor(B, 1)\n","            予測した確率分布のエントロピー．エントロピー正則化に使用．\n","        \"\"\"\n","        hidden = F.elu(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n","        hidden = F.elu(self.fc2(hidden))\n","        hidden = F.elu(self.fc3(hidden))\n","        hidden = F.elu(self.fc4(hidden))\n","        mean = self.mean(hidden)\n","        stddev = self.std(hidden)\n","\n","        mean = torch.tanh(mean)\n","        stddev = 2 * torch.sigmoid((stddev + self.init_stddev) / 2) + self.min_stddev\n","        if eval:\n","            action = mean\n","            return action, None, None\n","\n","        action_dist = td.Independent(TruncNormalDist(mean, stddev, -1, 1), 1)  # 行動をサンプリングする分布: p_{\\psi} (\\hat{a}_t | \\hat{z}_t)\n","        action = action_dist.sample()  # 行動: \\hat{a}_t\n","\n","        action_log_prob = action_dist.log_prob(action)\n","        action_entropy = action_dist.entropy()\n","\n","        return action, action_log_prob, action_entropy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JRdJzb-YdsW6"},"outputs":[],"source":["class Critic(nn.Module):\n","    def __init__(self, hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n","        super().__init__()\n","\n","        self.fc1 = nn.Linear(state_dim * num_classes + rnn_hidden_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n","        self.out = nn.Linear(hidden_dim, 1)\n","\n","    def forward(self, state: torch.tensor, rnn_hidden: torch.Tensor):\n","        \"\"\"\n","        確率的状態を入力として，価値関数(lambda target)の値を予測する．．\n","\n","        Parameters\n","        ----------\n","        state : torch.Tensor (B, state_dim * num_classes)\n","            確率的状態．\n","        rnn_hidden : torch.Tensor (B, rnn_hidden_dim)\n","            決定論的状態．\n","\n","        Returns\n","        -------\n","        value : torch.Tensor (B, 1)\n","            入力された状態に対する状態価値関数の予測値．\n","        \"\"\"\n","        hidden = F.elu(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n","        hidden = F.elu(self.fc2(hidden))\n","        hidden = F.elu(self.fc3(hidden))\n","        hidden = F.elu(self.fc4(hidden))\n","        mean = self.out(hidden)\n","\n","        return mean"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tmptCR67aYZc"},"outputs":[],"source":["class ReplayBuffer(object):\n","    \"\"\"\n","    RNNを用いて訓練するのに適したリプレイバッファ\n","    \"\"\"\n","    def __init__(self, capacity, observation_shape, action_dim):\n","        self.capacity = capacity\n","\n","        self.observations = np.zeros((capacity, *observation_shape), dtype=np.float32)\n","        self.actions = np.zeros((capacity, action_dim), dtype=np.float32)\n","        self.rewards = np.zeros((capacity, 1), dtype=np.float32)\n","        self.done = np.zeros((capacity, 1), dtype=bool)\n","\n","        self.index = 0\n","        self.is_filled = False\n","\n","    def push(self, observation, action, reward, done):\n","        \"\"\"\n","        リプレイバッファに経験を追加する\n","        \"\"\"\n","        self.observations[self.index] = observation\n","        self.actions[self.index] = action\n","        self.rewards[self.index] = reward\n","        self.done[self.index] = done\n","\n","        # indexは巡回し, 最も古い経験を上書きする\n","        if self.index == self.capacity - 1:\n","            self.is_filled = True\n","        self.index = (self.index + 1) % self.capacity\n","\n","    def sample(self, batch_size, chunk_length):\n","        \"\"\"\n","        経験をリプレイバッファからサンプルします. （ほぼ）一様なサンプルです\n","        結果として返ってくるのは観測(画像), 行動, 報酬, 終了シグナルについての(batch_size, chunk_length, 各要素の次元)の配列です\n","        各バッチは連続した経験になっています\n","        注意: chunk_lengthをあまり大きな値にすると問題が発生する場合があります\n","        \"\"\"\n","        episode_borders = np.where(self.done)[0]\n","        sampled_indexes = []\n","        for _ in range(batch_size):\n","            cross_border = True\n","            while cross_border:\n","                initial_index = np.random.randint(len(self) - chunk_length + 1)\n","                final_index = initial_index + chunk_length - 1\n","                cross_border = np.logical_and(initial_index <= episode_borders,\n","                                              episode_borders < final_index).any()#論理積\n","            sampled_indexes += list(range(initial_index, final_index + 1))\n","\n","        sampled_observations = self.observations[sampled_indexes].reshape(\n","            batch_size, chunk_length, *self.observations.shape[1:])\n","        sampled_actions = self.actions[sampled_indexes].reshape(\n","            batch_size, chunk_length, self.actions.shape[1])\n","        sampled_rewards = self.rewards[sampled_indexes].reshape(\n","            batch_size, chunk_length, 1)\n","        sampled_done = self.done[sampled_indexes].reshape(\n","            batch_size, chunk_length, 1)\n","        return sampled_observations, sampled_actions, sampled_rewards, sampled_done\n","\n","    def __len__(self):\n","        return self.capacity if self.is_filled else self.index\n","\n","    def save(self, dir: str):\n","        np.save(f\"{dir}/observations\", self.observations)\n","        np.save(f\"{dir}/actions\", self.actions)\n","        np.save(f\"{dir}/rewards\", self.rewards)\n","        np.save(f\"{dir}/done\", self.done)\n","\n","    def load(self, dir: str):\n","        self.observations = np.load(f\"{dir}/observations.npy\")\n","        self.actions = np.load(f\"{dir}/actions.npy\")\n","        self.rewards = np.load(f\"{dir}/rewards.npy\")\n","        self.done = np.load(f\"{dir}/done.npy\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MLIt0qiya1_2"},"outputs":[],"source":["def preprocess_obs(obs):\n","    \"\"\"\n","    画像の変換. [0, 255] -> [-0.5, 0.5]\n","    \"\"\"\n","    obs = obs.astype(np.float32)\n","    normalized_obs = obs / 255.0 - 0.5\n","    return normalized_obs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fLZMKJj_bIHr"},"outputs":[],"source":["def calculate_lambda_target(rewards: torch.Tensor, discounts: torch.Tensor, values: torch.Tensor, lambda_: float):\n","    \"\"\"\n","    lambda targetを計算する関数．\n","\n","    Parameters\n","    ---------\n","    rewards : torch.Tensor (imagination_horizon, D)\n","        報酬．1次元目が時刻tを表しており，2次元目は自由な次元数にでき，想像の軌道を作成するときに入力されるサンプルindexと考える．\n","    discounts : torch.Tensor (imagination_horizon, D)\n","        割引率．gammaそのままを利用するのではなく，DiscountModelの出力をかけて利用する．\n","    values : torch.Tensor (imagination_horizon, D)\n","        状態価値関数．criticで予測された値を利用するが，Dreamer v2ではtarget networkで計算する．\n","    lambda_ : float\n","        lambda targetのハイパラ．\n","\n","    Returns\n","    -------\n","    V_lambda : torch.Tensor (imagination_horizon, D)\n","        lambda targetの値．\n","    \"\"\"\n","    V_lambda = torch.zeros_like(rewards)\n","\n","    for t in reversed(range(rewards.shape[0])):\n","        if t == rewards.shape[0] - 1:\n","            V_lambda[t] = rewards[t] + discounts[t] * values[t]  # t=Hの場合（式4の下の条件）\n","        else:\n","            V_lambda[t] = rewards[t] + discounts[t] * ((1-lambda_) * values[t+1] + lambda_ * V_lambda[t+1])\n","\n","    return V_lambda"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kGY-YZzmSh-3"},"outputs":[],"source":["class Agent(nn.Module):\n","    \"\"\"\n","    ActionModelに基づき行動を決定する. そのためにRSSMを用いて状態表現をリアルタイムで推論して維持するクラス\n","    \"\"\"\n","    def __init__(self, encoder, decoder, rssm, action_model):\n","        super().__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.rssm = rssm\n","        self.action_model = action_model\n","\n","        self.device = next(self.action_model.parameters()).device\n","        self.rnn_hidden = torch.zeros(1, rssm.rnn_hidden_dim, device=self.device)\n","\n","    def __call__(self, obs, eval=True):\n","        # preprocessを適用, PyTorchのためにChannel-Firstに変換\n","        obs = preprocess_obs(obs)\n","        obs = torch.as_tensor(obs, device=self.device)\n","        obs = obs.transpose(1, 2).transpose(0, 1).unsqueeze(0)\n","\n","        with torch.no_grad():\n","            # 現在の状態から次に得られる観測画像を予測する\n","            state_prior = self.rssm.get_prior(self.rnn_hidden)\n","            state = state_prior.sample().flatten(1)\n","            obs_dist = self.decoder(state, self.rnn_hidden)\n","            obs_pred = obs_dist.mean\n","\n","            # 観測を低次元の表現に変換し, posteriorからのサンプルをActionModelに入力して行動を決定する\n","            embedded_obs = self.encoder(obs)\n","            state_posterior = self.rssm.get_posterior(self.rnn_hidden, embedded_obs)\n","            state = state_posterior.sample().flatten(1)\n","            action, _, _  = self.action_model(state, self.rnn_hidden, eval=eval)\n","\n","            # 次のステップのためにRNNの隠れ状態を更新しておく\n","            self.rnn_hidden = self.rssm.recurrent(state, action, self.rnn_hidden)\n","\n","        return action.squeeze().cpu().numpy(), (obs_pred.squeeze().cpu().numpy().transpose(1, 2, 0) + 0.5).clip(0.0, 1.0)\n","\n","    #RNNの隠れ状態をリセット\n","    def reset(self):\n","        self.rnn_hidden = torch.zeros(1, self.rssm.rnn_hidden_dim, device=self.device)\n","\n","    def to(self, device):\n","        self.device = device\n","        self.encoder.to(device)\n","        self.decoder.to(device)\n","        self.rssm.to(device)\n","        self.action_model.to(device)\n","        self.rnn_hidden = self.rnn_hidden.to(device)"]},{"cell_type":"markdown","metadata":{"id":"QUPWAkpyc9Z-"},"source":["## 5. モデルの学習  \n","**アルゴリズム・モデルに合わせて修正いただいて構いません．**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TMKhjMfcfvwi"},"outputs":[],"source":["class Config:\n","    def __init__(self, **kwargs):\n","        # コメントアウトされている値は，元実装のハイパーパラメータの値\n","        # data settings\n","        self.buffer_size = 100_000  # バッファにためるデータの上限\n","        self.batch_size = 16  # 50  # 学習時のバッチサイズ\n","        self.seq_length = 50  # 各バッチの系列長\n","        self.imagination_horizon = 10  # 15  # 想像上の軌道の系列長\n","\n","        # model dimensions\n","        self.state_dim = 20  # 32  # 確率的な状態の次元数\n","        self.num_classes = 20  # 32  # 確率的な状態のクラス数（離散表現のため）\n","        self.rnn_hidden_dim = 200  # 600  # 決定論的な状態の次元数\n","        self.mlp_hidden_dim = 200  # 400  # MLPの隠れ層の次元数\n","\n","        # learning params\n","        self.model_lr = 2e-4  # world model(transition / prior / posterior / discount / image predictor)の学習率\n","        self.actor_lr = 4e-5  # actorの学習率\n","        self.critic_lr = 1e-4  # criticの学習率\n","        self.epsilon = 1e-5  # optimizerのepsilonの値\n","        self.weight_decay = 1e-6  # weight decayの係数\n","        self.gradient_clipping = 100  # 勾配クリッピング\n","        self.kl_scale = 0.1  # kl lossのスケーリング係数\n","        self.kl_balance = 0.8  # kl balancingの係数(fix posterior)\n","        self.actor_entropy_scale = 1e-3  # entropy正則化のスケーリング係数\n","        self.slow_critic_update = 100  # target critic networkの更新頻度\n","        self.reward_loss_scale = 1.0  # reward lossのスケーリング係数\n","        self.discount_loss_scale = 1.0  # discount lossのスケーリング係数\n","        self.update_freq = 80  # 4\n","\n","        # lambda return params\n","        self.discount = 0.995  # 割引率\n","        self.lambda_ = 0.95  # lambda returnのパラメータ\n","\n","        # learning period settings\n","        self.iter = 600  # 総ステップ数（最初のランダム方策含む）\n","        self.seed_iter = 300  # 事前にランダム行動で探索する回数\n","        self.eval_freq = 5  # 評価頻度（エピソード）\n","        self.eval_episodes = 5  # 評価に用いるエピソード数\n","\n","cfg = Config()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z8sW8K5Kdjrg"},"outputs":[],"source":["set_seed(1234)  # PyTorchのシード固定\n","env = GymWrapperMetaWorld(\"hammer\", seed=0, size=(64, 64))\n","env = make_env(env, seed=0)\n","\n","eval_env = GymWrapperMetaWorld(\"hammer\", seed=0, size=(64, 64))\n","eval_env = make_env(eval_env, seed=1234)\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(device)\n","\n","action_dim = env.action_space.shape[0]\n","# リプレイバッファ\n","replay_buffer = ReplayBuffer(\n","    capacity=cfg.buffer_size,\n","    observation_shape=(64, 64, 3),\n","    action_dim=env.action_space.shape[0],\n",")\n","\n","# モデル\n","rssm = RSSM(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes, action_dim).to(device)\n","encoder = Encoder().to(device)\n","decoder = Decoder(cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n","reward_model =  RewardModel(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n","# discount_model = DiscountModel(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n","actor = Actor(action_dim, cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n","critic = Critic(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n","target_critic = Critic(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n","target_critic.load_state_dict(critic.state_dict())\n","\n","# optimizer\n","wm_params = list(rssm.parameters())         + \\\n","            list(encoder.parameters())      + \\\n","            list(decoder.parameters())      + \\\n","            list(reward_model.parameters()) # + \\\n","            # list(discount_model.parameters())\n","\n","wm_optimizer = torch.optim.Adam(wm_params, lr=cfg.model_lr, eps=cfg.epsilon, weight_decay=cfg.weight_decay)\n","actor_optimizer = torch.optim.Adam(actor.parameters(), lr=cfg.actor_lr, eps=cfg.epsilon, weight_decay=cfg.weight_decay)\n","critic_optimizer = torch.optim.Adam(critic.parameters(), lr=cfg.critic_lr, eps=cfg.epsilon, weight_decay=cfg.weight_decay)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1IzprukvoiOD"},"outputs":[],"source":["def evaluation(eval_env: RepeatAction, policy: Agent, step: int, cfg: Config):\n","    \"\"\"\n","    評価用の関数．\n","\n","    Parameters\n","    ----------\n","    policy : Agent\n","        エージェントのインスタンス．\n","    step : int\n","        現状の訓練のステップ数．\n","    cfg : Config\n","        コンフィグ．\n","\n","    Returns\n","    -------\n","    max_ep_rewards : float\n","        評価中に1エピソードで得た最大の報酬和．\n","    \"\"\"\n","    env = eval_env\n","    all_ep_rewards = []\n","\n","    with torch.no_grad():\n","        for i in range(cfg.eval_episodes):\n","            obs = env.reset()  # 環境をリセット\n","            policy.reset()  # RNNの隠れ状態をリセット\n","            done = False  # 終端条件\n","            truncated = False\n","            episode_reward = []  # エピソードでの報酬和\n","            while not done and not truncated:\n","                action, _ = policy(obs)\n","\n","                obs, reward, done, truncated, info = env.step(action)\n","                episode_reward.append(reward)\n","\n","            if len(episode_reward) < 500:\n","                if info[\"success\"]:\n","                    episode_reward = np.pad(episode_reward, (0, 500 - len(episode_reward)), \"constant\", constant_values=10)\n","                else:\n","                    episode_reward = np.pad(episode_reward, (0, 500 - len(episode_reward)), \"constant\", constant_values=0)\n","\n","            mean_episode_reward = np.mean(episode_reward)\n","            all_ep_rewards.append(mean_episode_reward)\n","\n","        mean_ep_rewards = np.mean(all_ep_rewards)\n","        max_ep_rewards = np.max(all_ep_rewards)\n","        print(f\"Eval(iter={step}) mean: {mean_ep_rewards:.4f} max: {max_ep_rewards:.4f}\")\n","\n","    return max_ep_rewards"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f8U7uuJgNgKz"},"outputs":[],"source":["# モデルの要件チェック\n","check_output_type(Agent(encoder, decoder, rssm, actor), env)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"txKI9CCne5e3"},"outputs":[],"source":["# ランダム行動でバッファを埋める\n","obs = env.reset()\n","done = False\n","for _ in range(cfg.seed_iter):\n","    action = env.action_space.sample()\n","    next_obs, reward, done, truncated, _ = env.step(action)\n","    done = done or truncated\n","\n","    if done or truncated:\n","        replay_buffer.push(preprocess_obs(obs), action, reward, done)\n","        obs = env.reset()\n","        done = False\n","        truncated = False\n","\n","    else:\n","        replay_buffer.push(preprocess_obs(obs), action, reward, done)\n","        obs = next_obs"]},{"cell_type":"code","source":["# pretrain\n","for iteration in range(100):\n","    # モデルの学習\n","    # リプレイバッファからデータをサンプリングする\n","    # (batch size, seq_lenght, *data shape)\n","    observations, actions, rewards, done_flags =\\\n","        replay_buffer.sample(cfg.batch_size, cfg.seq_length)\n","    done_flags = 1 - done_flags  # 終端でない場合に1をとる\n","\n","    # torchで扱える形（seq lengthを最初の次元に，画像はchnnelを最初の次元にする）に変形，observationの前処理\n","    observations = torch.permute(torch.as_tensor(observations, device=device), (1, 0, 4, 2, 3))  # (T, B, C, H, W)\n","    actions = torch.as_tensor(actions, device=device).transpose(0, 1)  # (T, B, action dim)\n","    rewards = torch.as_tensor(rewards, device=device).transpose(0, 1)  # (T, B, 1)\n","    done_flags = torch.as_tensor(done_flags, device=device).transpose(0, 1).float()  # (T, B, 1)\n","\n","    # =================\n","    # world modelの学習\n","    # =================\n","    # 観測をベクトルに埋めこみ\n","    emb_observations = encoder(observations.reshape(-1, 3, 64, 64)).view(cfg.seq_length, cfg.batch_size, -1)  # (T, B, 1536)\n","\n","    # 状態表現z，行動aはゼロで初期化\n","    # バッファから取り出したデータをt={1, ..., seq length}とするなら，以下はz_1とみなせる\n","    state = torch.zeros(cfg.batch_size, cfg.state_dim*cfg.num_classes, device=device)\n","    rnn_hidden = torch.zeros(cfg.batch_size, cfg.rnn_hidden_dim, device=device)\n","\n","    # 各観測に対して状態表現を計算\n","    # タイムステップごとに計算するため，先に格納するTensorを定義する(t={1, ..., seq length})\n","    states = torch.zeros(cfg.seq_length, cfg.batch_size, cfg.state_dim*cfg.num_classes, device=device)\n","    rnn_hiddens = torch.zeros(cfg.seq_length, cfg.batch_size, cfg.rnn_hidden_dim, device=device)\n","\n","    # prior, posteriorを計算してKL lossを計算する\n","    kl_loss = 0\n","    for i in range(cfg.seq_length-1):\n","        # rnn hiddenを更新\n","        rnn_hidden = rssm.recurrent(state, actions[i], rnn_hidden)  # h_t+1\n","\n","        # prior, posteriorを計算\n","        next_state_prior, next_detach_prior = rssm.get_prior(rnn_hidden, detach=True) # \\hat{z}_t+1\n","        next_state_posterior, next_detach_posterior = rssm.get_posterior(rnn_hidden, emb_observations[i+1], detach=True)  # z_t+1\n","\n","        # posteriorからzをサンプリング\n","        state = next_state_posterior.rsample().flatten(1)\n","        rnn_hiddens[i+1] = rnn_hidden  # h_t+1\n","        states[i+1] = state  # z_t+1\n","\n","        # KL lossを計算\n","        kl_loss +=  cfg.kl_balance * torch.mean(kl_divergence(next_detach_posterior, next_state_prior)) + \\\n","                    (1 - cfg.kl_balance) * torch.mean(kl_divergence(next_state_posterior, next_detach_prior))\n","    kl_loss /= (cfg.seq_length - 1)\n","\n","    # 初期状態は使わない\n","    rnn_hiddens = rnn_hiddens[1:]  # (seq lenghth - 1, batch size rnn hidden)\n","    states = states[1:]  # (seq length - 1, batch size, state dim * num_classes)\n","\n","    # 得られた状態を利用して再構成，報酬，終端フラグを予測\n","    # そのままでは時間方向，バッチ方向で次元が多いため平坦化\n","    flatten_rnn_hiddens = rnn_hiddens.view(-1, cfg.rnn_hidden_dim)  # ((T-1) * B, rnn hidden)\n","    flatten_states = states.view(-1, cfg.state_dim * cfg.num_classes)  # ((T-1) * B, state_dim * num_classes)\n","\n","    # 上から再構成，報酬，終端フラグ予測\n","    obs_dist = decoder(flatten_states, flatten_rnn_hiddens)  # (T * B, 3, 64, 64)\n","    reward_dist = reward_model(flatten_states, flatten_rnn_hiddens)  # (T * B, 1)\n","    # discount_dist = discount_model(flatten_states, flatten_rnn_hiddens)  # (T * B, 1)\n","\n","    # 各予測に対する損失の計算（対数尤度）\n","    C, H, W = observations.shape[2:]\n","    obs_loss = -torch.mean(obs_dist.log_prob(observations[1:].reshape(-1, C, H, W)))\n","    reward_loss = -torch.mean(reward_dist.log_prob(rewards[:-1].reshape(-1, 1)))\n","    # discount_loss = -torch.mean(discount_dist.log_prob(done_flags[:-1].float().reshape(-1, 1)))\n","\n","    # 総和をとってモデルを更新\n","    # wm_loss = obs_loss + cfg.reward_loss_scale * reward_loss + cfg.discount_loss_scale * discount_loss + cfg.kl_scale * kl_loss\n","    wm_loss = obs_loss + cfg.reward_loss_scale * reward_loss + cfg.kl_scale * kl_loss\n","\n","    wm_optimizer.zero_grad()\n","    wm_loss.backward()\n","    clip_grad_norm_(wm_params, cfg.gradient_clipping)\n","    wm_optimizer.step()\n","\n","    #====================\n","    # Actor, Criticの更新\n","    #===================\n","    # wmから得た状態の勾配を切っておく\n","    flatten_rnn_hiddens = flatten_rnn_hiddens.detach()\n","    flatten_states = flatten_states.detach()\n","\n","    # priorを用いた状態予測\n","    # 格納する空のTensorを用意\n","    imagined_states = torch.zeros(cfg.imagination_horizon + 1,\n","                                  *flatten_states.shape,\n","                                  device=flatten_states.device)\n","    imagined_rnn_hiddens = torch.zeros(cfg.imagination_horizon + 1,\n","                                       *flatten_rnn_hiddens.shape,\n","                                       device=flatten_rnn_hiddens.device)\n","    imagined_action_log_probs = torch.zeros((cfg.imagination_horizon, cfg.batch_size * (cfg.seq_length-1)),\n","                                            device=flatten_rnn_hiddens.device)\n","    imagined_action_entropys = torch.zeros((cfg.imagination_horizon, cfg.batch_size * (cfg.seq_length-1)),\n","                                            device=flatten_rnn_hiddens.device)\n","\n","    # 未来予測をして想像上の軌道を作る前に, 最初の状態としては先ほどモデルの更新で使っていた\n","    # リプレイバッファからサンプルされた観測データを取り込んだ上で推論した状態表現を使う\n","    imagined_states[0] = flatten_states\n","    imagined_rnn_hiddens[0] = flatten_rnn_hiddens\n","\n","    for i in range(1, cfg.imagination_horizon + 1):\n","        actions, action_log_probs, action_entropys = actor(flatten_states, flatten_rnn_hiddens)  # ((T-1) * B, action dim)\n","\n","        # rnn hiddenを更新, priorで次の状態を予測\n","        flatten_rnn_hiddens = rssm.recurrent(flatten_states, actions, flatten_rnn_hiddens)  # h_t+1\n","        flatten_states_prior = rssm.get_prior(flatten_rnn_hiddens)\n","        flatten_states = flatten_states_prior.rsample().flatten(1)\n","\n","        imagined_rnn_hiddens[i] = flatten_rnn_hiddens\n","        imagined_states[i] = flatten_states\n","        imagined_action_log_probs[i-1] = action_log_probs\n","        imagined_action_entropys[i-1] = action_entropys\n","\n","    imagined_states = imagined_states[1:]\n","    imagined_rnn_hiddens = imagined_rnn_hiddens[1:]\n","\n","    # 得られた状態から報酬を予測\n","    flatten_imagined_states = imagined_states.view(-1, cfg.state_dim * cfg.num_classes)  # ((imagination horizon) * (T-1) * B, state dim * num classes)\n","    flatten_imagined_rnn_hiddens = imagined_rnn_hiddens.view(-1, cfg.rnn_hidden_dim)  # ((imagination horizon) * (T-1) * B, rnn hidden)\n","\n","    # reward, done_flagsは分布なので平均値をとる\n","    # ((imagination horizon + 1), (T-1) * B)\n","    imagined_rewards = reward_model(flatten_imagined_states, flatten_imagined_rnn_hiddens).mean.view(cfg.imagination_horizon, -1)\n","    target_values = target_critic(flatten_imagined_states, flatten_imagined_rnn_hiddens).view(cfg.imagination_horizon, -1).detach()\n","    discount_arr = (cfg.discount * torch.ones_like(imagined_rewards)).to(device)\n","    initial_done = done_flags[1:].reshape(1, -1)\n","    discount_arr[0] = cfg.discount * initial_done\n","\n","    # lambda targetの計算\n","    lambda_target = calculate_lambda_target(imagined_rewards, discount_arr, target_values, cfg.lambda_)\n","\n","    # actorの損失を計算\n","    weights = torch.cumprod(\n","        torch.cat([torch.ones_like(discount_arr[:1]), discount_arr[:-1]], dim=0), dim=0\n","    )\n","    weights[-1] = 0.0\n","    objective = lambda_target + cfg.actor_entropy_scale * imagined_action_entropys\n","    actor_loss = -(weights * objective).mean()\n","\n","    actor_optimizer.zero_grad()\n","    actor_loss.backward()\n","    clip_grad_norm_(actor.parameters(), cfg.gradient_clipping)\n","    actor_optimizer.step()\n","\n","    # criticの損失を計算\n","    value_mean = critic(flatten_imagined_states.detach(), flatten_imagined_rnn_hiddens.detach()).view(cfg.imagination_horizon, -1)\n","    value_dist = MSE(value_mean)\n","    critic_loss = -(weights.detach() * value_dist.log_prob(lambda_target.detach())).mean()\n","\n","    critic_optimizer.zero_grad()\n","    critic_loss.backward()\n","    clip_grad_norm_(critic.parameters(), cfg.gradient_clipping)\n","    critic_optimizer.step()\n","\n","    if (iteration + 1) % cfg.slow_critic_update == 0:\n","        target_critic.load_state_dict(critic.state_dict())"],"metadata":{"id":"KPZGSHbkf1yo"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6_LNryZQfnKT"},"outputs":[],"source":["# 学習を行う\n","# 環境と相互作用 → 一定イテレーションでモデル更新を繰り返す\n","policy = Agent(encoder, decoder, rssm, actor)\n","\n","# 環境，収益等の初期化\n","obs = env.reset()\n","done = False\n","truncated = False\n","total_reward = []\n","total_episode = 1\n","best_reward = -1\n","\n","for iteration in range(cfg.iter):\n","    with torch.no_grad():\n","        # 環境と相互作用\n","        action, _ = policy(obs, eval=False)  # モデルで行動をサンプリング(one-hot)\n","        next_obs, reward, done,truncated, info = env.step(action)  # 環境を進める\n","        done = done or truncated\n","\n","        # 得たデータをリプレイバッファに追加して更新\n","        replay_buffer.push(preprocess_obs(obs), action, reward, done)  # x_t, a_t, r_t, gamma_t\n","        obs = next_obs\n","        total_reward.append(reward)\n","\n","    if (iteration + 1) % cfg.update_freq == 0:\n","        # モデルの学習\n","        # リプレイバッファからデータをサンプリングする\n","        # (batch size, seq_lenght, *data shape)\n","        observations, actions, rewards, done_flags =\\\n","            replay_buffer.sample(cfg.batch_size, cfg.seq_length)\n","        done_flags = 1 - done_flags  # 終端でない場合に1をとる\n","\n","        # torchで扱える形（seq lengthを最初の次元に，画像はchnnelを最初の次元にする）に変形，observationの前処理\n","        observations = torch.permute(torch.as_tensor(observations, device=device), (1, 0, 4, 2, 3))  # (T, B, C, H, W)\n","        actions = torch.as_tensor(actions, device=device).transpose(0, 1)  # (T, B, action dim)\n","        rewards = torch.as_tensor(rewards, device=device).transpose(0, 1)  # (T, B, 1)\n","        done_flags = torch.as_tensor(done_flags, device=device).transpose(0, 1).float()  # (T, B, 1)\n","\n","        # =================\n","        # world modelの学習\n","        # =================\n","        # 観測をベクトルに埋めこみ\n","        emb_observations = encoder(observations.reshape(-1, 3, 64, 64)).view(cfg.seq_length, cfg.batch_size, -1)  # (T, B, 1536)\n","\n","        # 状態表現z，行動aはゼロで初期化\n","        # バッファから取り出したデータをt={1, ..., seq length}とするなら，以下はz_1とみなせる\n","        state = torch.zeros(cfg.batch_size, cfg.state_dim*cfg.num_classes, device=device)\n","        rnn_hidden = torch.zeros(cfg.batch_size, cfg.rnn_hidden_dim, device=device)\n","\n","        # 各観測に対して状態表現を計算\n","        # タイムステップごとに計算するため，先に格納するTensorを定義する(t={1, ..., seq length})\n","        states = torch.zeros(cfg.seq_length, cfg.batch_size, cfg.state_dim*cfg.num_classes, device=device)\n","        rnn_hiddens = torch.zeros(cfg.seq_length, cfg.batch_size, cfg.rnn_hidden_dim, device=device)\n","\n","        # prior, posteriorを計算してKL lossを計算する\n","        kl_loss = 0\n","        for i in range(cfg.seq_length-1):\n","            # rnn hiddenを更新\n","            rnn_hidden = rssm.recurrent(state, actions[i], rnn_hidden)  # h_t+1\n","\n","            # prior, posteriorを計算\n","            next_state_prior, next_detach_prior = rssm.get_prior(rnn_hidden, detach=True) # \\hat{z}_t+1\n","            next_state_posterior, next_detach_posterior = rssm.get_posterior(rnn_hidden, emb_observations[i+1], detach=True)  # z_t+1\n","\n","            # posteriorからzをサンプリング\n","            state = next_state_posterior.rsample().flatten(1)\n","            rnn_hiddens[i+1] = rnn_hidden  # h_t+1\n","            states[i+1] = state  # z_t+1\n","\n","            # KL lossを計算\n","            kl_loss +=  cfg.kl_balance * torch.mean(kl_divergence(next_detach_posterior, next_state_prior)) + \\\n","                        (1 - cfg.kl_balance) * torch.mean(kl_divergence(next_state_posterior, next_detach_prior))\n","        kl_loss /= (cfg.seq_length - 1)\n","\n","        # 初期状態は使わない\n","        rnn_hiddens = rnn_hiddens[1:]  # (seq lenghth - 1, batch size rnn hidden)\n","        states = states[1:]  # (seq length - 1, batch size, state dim * num_classes)\n","\n","        # 得られた状態を利用して再構成，報酬，終端フラグを予測\n","        # そのままでは時間方向，バッチ方向で次元が多いため平坦化\n","        flatten_rnn_hiddens = rnn_hiddens.view(-1, cfg.rnn_hidden_dim)  # ((T-1) * B, rnn hidden)\n","        flatten_states = states.view(-1, cfg.state_dim * cfg.num_classes)  # ((T-1) * B, state_dim * num_classes)\n","\n","        # 上から再構成，報酬，終端フラグ予測\n","        obs_dist = decoder(flatten_states, flatten_rnn_hiddens)  # (T * B, 3, 64, 64)\n","        reward_dist = reward_model(flatten_states, flatten_rnn_hiddens)  # (T * B, 1)\n","        # discount_dist = discount_model(flatten_states, flatten_rnn_hiddens)  # (T * B, 1)\n","\n","        # 各予測に対する損失の計算（対数尤度）\n","        C, H, W = observations.shape[2:]\n","        obs_loss = -torch.mean(obs_dist.log_prob(observations[1:].reshape(-1, C, H, W)))\n","        reward_loss = -torch.mean(reward_dist.log_prob(rewards[:-1].reshape(-1, 1)))\n","        # discount_loss = -torch.mean(discount_dist.log_prob(done_flags[:-1].float().reshape(-1, 1)))\n","\n","        # 総和をとってモデルを更新\n","        # wm_loss = obs_loss + cfg.reward_loss_scale * reward_loss + cfg.discount_loss_scale * discount_loss + cfg.kl_scale * kl_loss\n","        wm_loss = obs_loss + cfg.reward_loss_scale * reward_loss + cfg.kl_scale * kl_loss\n","\n","        wm_optimizer.zero_grad()\n","        wm_loss.backward()\n","        clip_grad_norm_(wm_params, cfg.gradient_clipping)\n","        wm_optimizer.step()\n","\n","        #====================\n","        # Actor, Criticの更新\n","        #===================\n","        # wmから得た状態の勾配を切っておく\n","        flatten_rnn_hiddens = flatten_rnn_hiddens.detach()\n","        flatten_states = flatten_states.detach()\n","\n","        # priorを用いた状態予測\n","        # 格納する空のTensorを用意\n","        imagined_states = torch.zeros(cfg.imagination_horizon + 1,\n","                                      *flatten_states.shape,\n","                                      device=flatten_states.device)\n","        imagined_rnn_hiddens = torch.zeros(cfg.imagination_horizon + 1,\n","                                           *flatten_rnn_hiddens.shape,\n","                                           device=flatten_rnn_hiddens.device)\n","        imagined_action_log_probs = torch.zeros((cfg.imagination_horizon, cfg.batch_size * (cfg.seq_length-1)),\n","                                                device=flatten_rnn_hiddens.device)\n","        imagined_action_entropys = torch.zeros((cfg.imagination_horizon, cfg.batch_size * (cfg.seq_length-1)),\n","                                                device=flatten_rnn_hiddens.device)\n","\n","        # 未来予測をして想像上の軌道を作る前に, 最初の状態としては先ほどモデルの更新で使っていた\n","        # リプレイバッファからサンプルされた観測データを取り込んだ上で推論した状態表現を使う\n","        imagined_states[0] = flatten_states\n","        imagined_rnn_hiddens[0] = flatten_rnn_hiddens\n","\n","        for i in range(1, cfg.imagination_horizon + 1):\n","            actions, action_log_probs, action_entropys = actor(flatten_states, flatten_rnn_hiddens)  # ((T-1) * B, action dim)\n","\n","            # rnn hiddenを更新, priorで次の状態を予測\n","            flatten_rnn_hiddens = rssm.recurrent(flatten_states, actions, flatten_rnn_hiddens)  # h_t+1\n","            flatten_states_prior = rssm.get_prior(flatten_rnn_hiddens)\n","            flatten_states = flatten_states_prior.rsample().flatten(1)\n","\n","            imagined_rnn_hiddens[i] = flatten_rnn_hiddens\n","            imagined_states[i] = flatten_states\n","            imagined_action_log_probs[i-1] = action_log_probs\n","            imagined_action_entropys[i-1] = action_entropys\n","\n","        imagined_states = imagined_states[1:]\n","        imagined_rnn_hiddens = imagined_rnn_hiddens[1:]\n","\n","        # 得られた状態から報酬を予測\n","        flatten_imagined_states = imagined_states.view(-1, cfg.state_dim * cfg.num_classes)  # ((imagination horizon) * (T-1) * B, state dim * num classes)\n","        flatten_imagined_rnn_hiddens = imagined_rnn_hiddens.view(-1, cfg.rnn_hidden_dim)  # ((imagination horizon) * (T-1) * B, rnn hidden)\n","\n","        # reward, done_flagsは分布なので平均値をとる\n","        # ((imagination horizon + 1), (T-1) * B)\n","        imagined_rewards = reward_model(flatten_imagined_states, flatten_imagined_rnn_hiddens).mean.view(cfg.imagination_horizon, -1)\n","        target_values = target_critic(flatten_imagined_states, flatten_imagined_rnn_hiddens).view(cfg.imagination_horizon, -1).detach()\n","        discount_arr = (cfg.discount * torch.ones_like(imagined_rewards)).to(device)\n","        initial_done = done_flags[1:].reshape(1, -1)\n","        discount_arr[0] = cfg.discount * initial_done\n","\n","        # lambda targetの計算\n","        lambda_target = calculate_lambda_target(imagined_rewards, discount_arr, target_values, cfg.lambda_)\n","\n","        # actorの損失を計算\n","        weights = torch.cumprod(\n","            torch.cat([torch.ones_like(discount_arr[:1]), discount_arr[:-1]], dim=0), dim=0\n","        )\n","        weights[-1] = 0.0\n","        objective = lambda_target + cfg.actor_entropy_scale * imagined_action_entropys\n","        actor_loss = -(weights * objective).mean()\n","\n","        actor_optimizer.zero_grad()\n","        actor_loss.backward()\n","        clip_grad_norm_(actor.parameters(), cfg.gradient_clipping)\n","        actor_optimizer.step()\n","\n","        # criticの損失を計算\n","        value_mean = critic(flatten_imagined_states.detach(), flatten_imagined_rnn_hiddens.detach()).view(cfg.imagination_horizon, -1)\n","        value_dist = MSE(value_mean)\n","        critic_loss = -(weights.detach() * value_dist.log_prob(lambda_target.detach())).mean()\n","\n","        critic_optimizer.zero_grad()\n","        critic_loss.backward()\n","        clip_grad_norm_(critic.parameters(), cfg.gradient_clipping)\n","        critic_optimizer.step()\n","\n","        if (iteration + 1) % cfg.slow_critic_update == 0:\n","            target_critic.load_state_dict(critic.state_dict())\n","\n","    # エピソードが終了した時に再初期化\n","    if done or truncated:\n","        if len(total_reward) < 500:\n","            if info[\"success\"]:\n","                total_reward = np.pad(total_reward, (0, 500 - len(total_reward)), \"constant\", constant_values=10)\n","            else:\n","                total_reward = np.pad(total_reward, (0, 500 - len(total_reward)), \"constant\", constant_values=0)\n","\n","        mean_episode_reward = np.mean(total_reward)\n","        print(f\"episode: {total_episode} mean_episode_reward: {mean_episode_reward:.8f}\")\n","        print(f\"num iter: {iteration} kl loss: {kl_loss.item():.8f} obs loss: {obs_loss.item():.8f} \"\n","              f\"rewrd loss: {reward_loss.item():.8f} \" # discount loss: {discount_loss.item():.8f} \"\n","              f\"critic loss: {critic_loss.item():.8f} actor loss: {actor_loss.item():.8f}\"\n","        )\n","        obs = env.reset()\n","        done = False\n","        truncated = False\n","        total_reward = []\n","        total_episode += 1\n","        policy.reset()\n","\n","        # 一定エピソードごとに評価\n","        if total_episode % cfg.eval_freq == 0:\n","            eval_reward = evaluation(eval_env, policy, iteration, cfg)\n","            eval_env.reset()\n","            policy.reset()"]},{"cell_type":"markdown","metadata":{"id":"QY1cyyotT0K2"},"source":["## 6. エージェントの保存\n","- 保存する際には，CPU に移してからモデル全体を保存してください．\n","  - state_dict のみの保存に変更しないでください．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UWjmC-iMAMOg"},"outputs":[],"source":["agent = Agent(encoder, decoder, rssm, actor)  # この行は書き換え可\n","agent.to(\"cpu\")\n","torch.save(agent, \"agent.pth\")"]},{"cell_type":"markdown","metadata":{"id":"5qaekGawqhzL"},"source":["## 7. sutudent_code.py の作成\n","- `%%writefile student_code.py` はセルの内容を `student_code.py` としてファイル作成するマジックコマンドです．この部分は削除・変更等しないでください．\n","- 学習した Agent のクラスを定義したスクリプトを `student_code.py` として作成する必要があります．\n","- Agent クラス自体と，その内部で動作するクラス・関数をすべて含めてください．\n","- 各クラス・関数に利用されているライブラリもスクリプト内で import してください．\n","    - 不足しているクラス・関数がある場合，secure_submit.py を用いて提出物を作成する際にエラーが発生します．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qHl8zUIaV89U"},"outputs":[],"source":["%%writefile student_code.py\n","import numpy as np\n","import torch\n","import torch.distributions as td\n","from torch.distributions import Normal, OneHotCategoricalStraightThrough\n","from torch import nn\n","from torch.nn import functional as F\n","\n","\n","class MSE(td.Normal):\n","    def __init__(self, loc, validate_args=None):\n","        super(MSE, self).__init__(loc, 1.0, validate_args=validate_args)\n","\n","    @property\n","    def mode(self):\n","        return self.mean\n","\n","    def sample(self, sample_shape=torch.Size()):\n","        return self.rsample(sample_shape)\n","\n","    def log_prob(self, value):\n","        if self._validate_args:\n","            self._validate_sample(value)\n","        # NOTE: dropped the constant term\n","        return -((value - self.loc) ** 2) / 2\n","\n","# From https://github.com/toshas/torch_truncnorm/blob/main/TruncatedNormal.py\n","import math\n","from numbers import Number\n","\n","import torch\n","from torch.distributions import Distribution, constraints\n","from torch.distributions.utils import broadcast_all\n","\n","CONST_SQRT_2 = math.sqrt(2)\n","CONST_INV_SQRT_2PI = 1 / math.sqrt(2 * math.pi)\n","CONST_INV_SQRT_2 = 1 / math.sqrt(2)\n","CONST_LOG_INV_SQRT_2PI = math.log(CONST_INV_SQRT_2PI)\n","CONST_LOG_SQRT_2PI_E = 0.5 * math.log(2 * math.pi * math.e)\n","\n","\n","class TruncatedStandardNormal(Distribution):\n","    \"\"\"\n","    Truncated Standard Normal distribution\n","    https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n","    \"\"\"\n","\n","    arg_constraints = {\n","        'a': constraints.real,\n","        'b': constraints.real,\n","    }\n","    has_rsample = True\n","\n","    def __init__(self, a, b, validate_args=None):\n","        self.a, self.b = broadcast_all(a, b)\n","        if isinstance(a, Number) and isinstance(b, Number):\n","            batch_shape = torch.Size()\n","        else:\n","            batch_shape = self.a.size()\n","        super(TruncatedStandardNormal, self).__init__(batch_shape, validate_args=validate_args)\n","        if self.a.dtype != self.b.dtype:\n","            raise ValueError('Truncation bounds types are different')\n","        if any((self.a >= self.b).view(-1, ).tolist()):\n","            raise ValueError('Incorrect truncation range')\n","        eps = torch.finfo(self.a.dtype).eps\n","        self._dtype_min_gt_0 = eps\n","        self._dtype_max_lt_1 = 1 - eps\n","        self._little_phi_a = self._little_phi(self.a)\n","        self._little_phi_b = self._little_phi(self.b)\n","        self._big_phi_a = self._big_phi(self.a)\n","        self._big_phi_b = self._big_phi(self.b)\n","        self._Z = (self._big_phi_b - self._big_phi_a).clamp_min(eps)\n","        self._log_Z = self._Z.log()\n","        little_phi_coeff_a = torch.nan_to_num(self.a, nan=math.nan)\n","        little_phi_coeff_b = torch.nan_to_num(self.b, nan=math.nan)\n","        self._lpbb_m_lpaa_d_Z = (self._little_phi_b * little_phi_coeff_b -\n","                                 self._little_phi_a * little_phi_coeff_a) / self._Z\n","        self._mean = -(self._little_phi_b - self._little_phi_a) / self._Z\n","        # NOTE: additional to github.com/toshas/torch_truncnorm\n","        self._mode = torch.clamp(torch.zeros_like(self.a), self.a, self.b)\n","        self._variance = 1 - self._lpbb_m_lpaa_d_Z - ((self._little_phi_b - self._little_phi_a) / self._Z) ** 2\n","        self._entropy = CONST_LOG_SQRT_2PI_E + self._log_Z - 0.5 * self._lpbb_m_lpaa_d_Z\n","\n","    @constraints.dependent_property\n","    def support(self):\n","        return constraints.interval(self.a, self.b)\n","\n","    @property\n","    def mean(self):\n","        return self._mean\n","\n","    @property\n","    def mode(self):\n","        return self._mode\n","\n","    @property\n","    def variance(self):\n","        return self._variance\n","\n","    def entropy(self):\n","        return self._entropy\n","\n","    @property\n","    def auc(self):\n","        return self._Z\n","\n","    @staticmethod\n","    def _little_phi(x):\n","        return (-(x ** 2) * 0.5).exp() * CONST_INV_SQRT_2PI\n","\n","    @staticmethod\n","    def _big_phi(x):\n","        return 0.5 * (1 + (x * CONST_INV_SQRT_2).erf())\n","\n","    @staticmethod\n","    def _inv_big_phi(x):\n","        return CONST_SQRT_2 * (2 * x - 1).erfinv()\n","\n","    def cdf(self, value):\n","        if self._validate_args:\n","            self._validate_sample(value)\n","        return ((self._big_phi(value) - self._big_phi_a) / self._Z).clamp(0, 1)\n","\n","    def icdf(self, value):\n","        return self._inv_big_phi(self._big_phi_a + value * self._Z)\n","\n","    def log_prob(self, value):\n","        if self._validate_args:\n","            self._validate_sample(value)\n","        return CONST_LOG_INV_SQRT_2PI - self._log_Z - (value ** 2) * 0.5\n","\n","    def rsample(self, sample_shape=torch.Size()):\n","        # icdf is numerically unstable; as a consequence, so is rsample.\n","        shape = self._extended_shape(sample_shape)\n","        p = torch.empty(shape, device=self.a.device).uniform_(self._dtype_min_gt_0, self._dtype_max_lt_1)\n","        return self.icdf(p)\n","\n","\n","class TruncatedNormal(TruncatedStandardNormal):\n","    \"\"\"\n","    Truncated Normal distribution\n","    https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n","    \"\"\"\n","\n","    has_rsample = True\n","\n","    def __init__(self, loc, scale, scalar_a, scalar_b, validate_args=None):\n","        self.loc, self.scale, a, b = broadcast_all(loc, scale, scalar_a, scalar_b)\n","        a = (a - self.loc) / self.scale\n","        b = (b - self.loc) / self.scale\n","        super(TruncatedNormal, self).__init__(a, b, validate_args=validate_args)\n","        self._log_scale = self.scale.log()\n","        self._mean = self._mean * self.scale + self.loc\n","        self._mode = torch.clamp(self.loc, scalar_a, scalar_b)  # NOTE: additional to github.com/toshas/torch_truncnorm\n","        self._variance = self._variance * self.scale ** 2\n","        self._entropy += self._log_scale\n","\n","    def _to_std_rv(self, value):\n","        return (value - self.loc) / self.scale\n","\n","    def _from_std_rv(self, value):\n","        return value * self.scale + self.loc\n","\n","    def cdf(self, value):\n","        return super(TruncatedNormal, self).cdf(self._to_std_rv(value))\n","\n","    def icdf(self, value):\n","        return self._from_std_rv(super(TruncatedNormal, self).icdf(value))\n","\n","    def log_prob(self, value):\n","        return super(TruncatedNormal, self).log_prob(self._to_std_rv(value)) - self._log_scale\n","\n","\n","class TruncNormalDist(TruncatedNormal):\n","\n","    def __init__(self, loc, scale, low, high, clip=1e-6, mult=1):\n","        super().__init__(loc, scale, low, high)\n","        self._clip = clip\n","        self._mult = mult\n","\n","        self.low = low\n","        self.high = high\n","\n","    def sample(self, *args, **kwargs):\n","        event = super().rsample(*args, **kwargs)\n","        if self._clip:\n","            clipped = torch.clamp(\n","                event, self.low + self._clip, self.high - self._clip\n","            )\n","            event = event - event.detach() + clipped.detach()\n","        if self._mult:\n","            event *= self._mult\n","        return event\n","\n","\n","class RSSM(nn.Module):\n","    def __init__(self, mlp_hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int, actino_dim: int):\n","        super().__init__()\n","\n","        self.rnn_hidden_dim = rnn_hidden_dim\n","        self.state_dim = state_dim\n","        self.num_classes = num_classes\n","\n","        # Recurrent model\n","        # h_t = f(h_t-1, z_t-1, a_t-1)\n","        self.transition_hidden = nn.Linear(state_dim * num_classes + action_dim, mlp_hidden_dim)\n","        self.transition = nn.GRUCell(mlp_hidden_dim, rnn_hidden_dim)\n","\n","        # transition predictor\n","        self.prior_hidden = nn.Linear(rnn_hidden_dim, mlp_hidden_dim)\n","        self.prior_logits = nn.Linear(mlp_hidden_dim, state_dim * num_classes)\n","\n","        # representation model\n","        self.posterior_hidden = nn.Linear(rnn_hidden_dim + 1536, mlp_hidden_dim)\n","        self.posterior_logits = nn.Linear(mlp_hidden_dim, state_dim * num_classes)\n","\n","    def recurrent(self, state: torch.Tensor, action: torch.Tensor, rnn_hidden: torch.Tensor):\n","        # recullent model: h_t = f(h_t-1, z_t-1, a_t-1)を計算する\n","        hidden = F.elu(self.transition_hidden(torch.cat([state, action], dim=1)))\n","        rnn_hidden = self.transition(hidden, rnn_hidden)\n","\n","        return rnn_hidden  # h_t\n","\n","    def get_prior(self, rnn_hidden: torch.Tensor, detach=False):\n","        # transition predictor: \\hat{z}_t ~ p(z\\hat{z}_t | h_t)\n","        hidden = F.elu(self.prior_hidden(rnn_hidden))\n","        logits = self.prior_logits(hidden)\n","        logits = logits.reshape(logits.shape[0], self.state_dim, self.num_classes)\n","\n","        prior_dist = td.Independent(OneHotCategoricalStraightThrough(logits=logits), 1)\n","        if detach:\n","            detach_prior = td.Independent(OneHotCategoricalStraightThrough(logits=logits.detach()), 1)\n","            return prior_dist, detach_prior  # p(z\\hat{z}_t | h_t)\n","        return prior_dist\n","\n","    def get_posterior(self, rnn_hidden: torch.Tensor, embedded_obs: torch.Tensor, detach=False):\n","        # representation predictor: z_t ~ q(z_t | h_t, o_t)\n","        hidden = F.elu(self.posterior_hidden(torch.cat([rnn_hidden, embedded_obs], dim=1)))\n","        logits = self.posterior_logits(hidden)\n","        logits = logits.reshape(logits.shape[0], self.state_dim, self.num_classes)\n","\n","        posterior_dist = td.Independent(OneHotCategoricalStraightThrough(logits=logits), 1)\n","        if detach:\n","            detach_posterior = td.Independent(OneHotCategoricalStraightThrough(logits=logits.detach()), 1)\n","            return posterior_dist, detach_posterior  # q(z_t | h_t, o_t)\n","        return posterior_dist\n","\n","\n","class Encoder(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.conv1 = nn.Conv2d(3, 48, kernel_size=4, stride=2)\n","        self.conv2 = nn.Conv2d(48, 96, kernel_size=4, stride=2)\n","        self.conv3 = nn.Conv2d(96, 192, kernel_size=4, stride=2)\n","        self.conv4 = nn.Conv2d(192, 384, kernel_size=4, stride=2)\n","\n","    def forward(self, obs: torch.Tensor):\n","        \"\"\"\n","        観測画像をベクトルに埋め込むためのEncoder．\n","\n","        Parameters\n","        ----------\n","        obs : torch.Tensor (B, C, H, W)\n","            入力となる観測画像．\n","\n","        Returns\n","        -------\n","        embedded_obs : torch.Tensor (B, D)\n","            観測画像をベクトルに変換したもの．Dは入力画像の幅と高さに依存して変わる．\n","            入力が(B, 3, 64, 64)の場合，出力は(B, 1536)になる．\n","        \"\"\"\n","        hidden = F.elu(self.conv1(obs))\n","        hidden = F.elu(self.conv2(hidden))\n","        hidden = F.elu(self.conv3(hidden))\n","        embedded_obs = self.conv4(hidden).reshape(hidden.size(0), -1)\n","\n","        return embedded_obs  # x_t\n","\n","\n","class Encoder(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.conv1 = nn.Conv2d(3, 48, kernel_size=4, stride=2)\n","        self.conv2 = nn.Conv2d(48, 96, kernel_size=4, stride=2)\n","        self.conv3 = nn.Conv2d(96, 192, kernel_size=4, stride=2)\n","        self.conv4 = nn.Conv2d(192, 384, kernel_size=4, stride=2)\n","\n","    def forward(self, obs: torch.Tensor):\n","        \"\"\"\n","        観測画像をベクトルに埋め込むためのEncoder．\n","\n","        Parameters\n","        ----------\n","        obs : torch.Tensor (B, C, H, W)\n","            入力となる観測画像．\n","\n","        Returns\n","        -------\n","        embedded_obs : torch.Tensor (B, D)\n","            観測画像をベクトルに変換したもの．Dは入力画像の幅と高さに依存して変わる．\n","            入力が(B, 3, 64, 64)の場合，出力は(B, 1536)になる．\n","        \"\"\"\n","        hidden = F.elu(self.conv1(obs))\n","        hidden = F.elu(self.conv2(hidden))\n","        hidden = F.elu(self.conv3(hidden))\n","        embedded_obs = self.conv4(hidden).reshape(hidden.size(0), -1)\n","\n","        return embedded_obs  # x_t\n","\n","\n","class Decoder(nn.Module):\n","    def __init__(self, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n","        super().__init__()\n","        self.fc = nn.Linear(state_dim*num_classes + rnn_hidden_dim, 1536)\n","        self.dc1 = nn.ConvTranspose2d(1536, 192, kernel_size=5, stride=2)\n","        self.dc2 = nn.ConvTranspose2d(192, 96, kernel_size=5, stride=2)\n","        self.dc3 = nn.ConvTranspose2d(96, 48, kernel_size=6, stride=2)\n","        self.dc4 = nn.ConvTranspose2d(48, 3, kernel_size=6, stride=2)\n","\n","\n","    def forward(self, state: torch.Tensor, rnn_hidden: torch.Tensor):\n","        \"\"\"\n","        決定論的状態と，確率的状態を入力として，観測画像を復元するDecoder．\n","        出力は多次元正規分布の平均値をとる．\n","\n","        Paremters\n","        ---------\n","        state : torch.Tensor (B, state_dim * num_classes)\n","            確率的状態．\n","        rnn_hidden : torch.Tensor (B, rnn_hidden_dim)\n","            決定論的状態．\n","\n","        Returns\n","        -------\n","        obs_dist : torch.distribution.Independent\n","            観測画像を再構成するための多次元正規分布．\n","        \"\"\"\n","        hidden = self.fc(torch.cat([state, rnn_hidden], dim=1))\n","        hidden = hidden.view(hidden.size(0), 1536, 1, 1)\n","        hidden = F.elu(self.dc1(hidden))\n","        hidden = F.elu(self.dc2(hidden))\n","        hidden = F.elu(self.dc3(hidden))\n","        mean = self.dc4(hidden)\n","\n","        obs_dist = td.Independent(MSE(mean), 3)\n","        return obs_dist  # p(\\hat{x}_t | h_t, z_t)\n","\n","\n","class RewardModel(nn.Module):\n","    def __init__(self, hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n","        super().__init__()\n","        self.fc1 = nn.Linear(state_dim*num_classes + rnn_hidden_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc4 = nn.Linear(hidden_dim, 1)\n","\n","    def forward(self, state: torch.Tensor, rnn_hidden: torch.Tensor):\n","        \"\"\"\n","        決定論的状態と，確率的状態を入力として，報酬を予測するモデル．\n","        出力は正規分布の平均値をとる．\n","\n","        Paremters\n","        ---------\n","        state : torch.Tensor (B, state_dim * num_classes)\n","            確率的状態．\n","        rnn_hidden : torch.Tensor (B, rnn_hidden_dim)\n","            決定論的状態．\n","\n","        Returns\n","        -------\n","        reward_dist : torch.distribution.Independent\n","            報酬を予測するための正規分布．\n","        \"\"\"\n","        hidden = F.elu(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n","        hidden = F.elu(self.fc2(hidden))\n","        hidden = F.elu(self.fc3(hidden))\n","        mean = self.fc4(hidden)\n","\n","        reward_dist = td.Independent(MSE(mean),  1)\n","        return reward_dist  # p(\\hat{r}_t | h_t, z_t)\n","\n","\n","class DiscountModel(nn.Module):\n","    def __init__(self, hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n","        super().__init__()\n","        self.fc1 = nn.Linear(state_dim*num_classes + rnn_hidden_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc4 = nn.Linear(hidden_dim, 1)\n","\n","    def forward(self, state: torch.Tensor, rnn_hidden: torch.Tensor):\n","        \"\"\"\n","        決定論的状態と，確率的状態を入力として，現在の状態がエピソード終端かどうか判別するモデル．\n","        出力はベルヌーイ分布の平均値をとる．\n","\n","        Paremters\n","        ---------\n","        state : torch.Tensor (B, state_dim * num_classes)\n","            確率的状態．\n","        rnn_hidden : torch.Tensor (B, rnn_hidden_dim)\n","            決定論的状態．\n","\n","        Returns\n","        -------\n","        discount_dist : torch.distribution.Independent\n","            状態が終端かどうかを予測するためのベルヌーイ分布．\n","        \"\"\"\n","        hidden = F.elu(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n","        hidden = F.elu(self.fc2(hidden))\n","        hidden = F.elu(self.fc3(hidden))\n","        mean= self.fc4(hidden)\n","\n","        discount_dist = td.Independent(td.Bernoulli(logits=mean),  1)\n","        return discount_dist  # p(\\hat{\\gamma}_t | h_t, z_t)\n","\n","\n","class Actor(nn.Module):\n","    def __init__(self, action_dim: int, hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n","        super().__init__()\n","\n","        self.fc1 = nn.Linear(state_dim * num_classes + rnn_hidden_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n","        self.mean = nn.Linear(hidden_dim, action_dim)\n","        self.std = nn.Linear(hidden_dim, action_dim)\n","        self.min_stddev = 0.1\n","        self.init_stddev = np.log(np.exp(5.0) - 1)\n","\n","    def forward(self, state: torch.tensor, rnn_hidden: torch.Tensor, eval: bool = False):\n","        \"\"\"\n","        確率的状態を入力として，criticで推定される価値が最大となる行動を出力する．\n","\n","        Parameters\n","        ----------\n","        state : torch.Tensor (B, state_dim * num_classes)\n","            確率的状態．\n","        rnn_hidden : torch.Tensor (B, rnn_hidden_dim)\n","            決定論的状態．\n","\n","        Returns\n","        -------\n","        action : torch.Tensor (B, 1)\n","            行動．\n","        action_log_prob : torch.Tensor(B, 1)\n","            予測した行動をとる確率の対数．\n","        action_entropy : torch.Tensor(B, 1)\n","            予測した確率分布のエントロピー．エントロピー正則化に使用．\n","        \"\"\"\n","        hidden = F.elu(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n","        hidden = F.elu(self.fc2(hidden))\n","        hidden = F.elu(self.fc3(hidden))\n","        hidden = F.elu(self.fc4(hidden))\n","        mean = self.mean(hidden)\n","        stddev = self.std(hidden)\n","\n","        mean = torch.tanh(mean)\n","        stddev = 2 * torch.sigmoid((stddev + self.init_stddev) / 2) + self.min_stddev\n","        if eval:\n","            action = mean\n","            return action, None, None\n","\n","        action_dist = td.Independent(TruncNormalDist(mean, stddev, -1, 1), 1)  # 行動をサンプリングする分布: p_{\\psi} (\\hat{a}_t | \\hat{z}_t)\n","        action = action_dist.sample()  # 行動: \\hat{a}_t\n","\n","        action_log_prob = action_dist.log_prob(action)\n","        action_entropy = action_dist.entropy()\n","\n","        return action, action_log_prob, action_entropy\n","\n","\n","class Critic(nn.Module):\n","    def __init__(self, hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n","        super().__init__()\n","\n","        self.fc1 = nn.Linear(state_dim * num_classes + rnn_hidden_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n","        self.out = nn.Linear(hidden_dim, 1)\n","\n","    def forward(self, state: torch.tensor, rnn_hidden: torch.Tensor):\n","        \"\"\"\n","        確率的状態を入力として，価値関数(lambda target)の値を予測する．．\n","\n","        Parameters\n","        ----------\n","        state : torch.Tensor (B, state_dim * num_classes)\n","            確率的状態．\n","        rnn_hidden : torch.Tensor (B, rnn_hidden_dim)\n","            決定論的状態．\n","\n","        Returns\n","        -------\n","        value : torch.Tensor (B, 1)\n","            入力された状態に対する状態価値関数の予測値．\n","        \"\"\"\n","        hidden = F.elu(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n","        hidden = F.elu(self.fc2(hidden))\n","        hidden = F.elu(self.fc3(hidden))\n","        hidden = F.elu(self.fc4(hidden))\n","        mean = self.out(hidden)\n","\n","        return mean\n","\n","\n","class Agent(nn.Module):\n","    \"\"\"\n","    ActionModelに基づき行動を決定する. そのためにRSSMを用いて状態表現をリアルタイムで推論して維持するクラス\n","    \"\"\"\n","    def __init__(self, encoder, decoder, rssm, action_model):\n","        super().__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.rssm = rssm\n","        self.action_model = action_model\n","\n","        self.device = next(self.action_model.parameters()).device\n","        self.rnn_hidden = torch.zeros(1, rssm.rnn_hidden_dim, device=self.device)\n","\n","    def __call__(self, obs, eval=True):\n","        # preprocessを適用, PyTorchのためにChannel-Firstに変換\n","        obs = preprocess_obs(obs)\n","        obs = torch.as_tensor(obs, device=self.device)\n","        obs = obs.transpose(1, 2).transpose(0, 1).unsqueeze(0)\n","\n","        with torch.no_grad():\n","            # 現在の状態から次に得られる観測画像を予測する\n","            state_prior = self.rssm.get_prior(self.rnn_hidden)\n","            state = state_prior.sample().flatten(1)\n","            obs_dist = self.decoder(state, self.rnn_hidden)\n","            obs_pred = obs_dist.sample()\n","\n","            # 観測を低次元の表現に変換し, posteriorからのサンプルをActionModelに入力して行動を決定する\n","            embedded_obs = self.encoder(obs)\n","            state_posterior = self.rssm.get_posterior(self.rnn_hidden, embedded_obs)\n","            state = state_posterior.sample().flatten(1)\n","            action, _, _  = self.action_model(state, self.rnn_hidden, eval=eval)\n","\n","            # 次のステップのためにRNNの隠れ状態を更新しておく\n","            self.rnn_hidden = self.rssm.recurrent(state, action, self.rnn_hidden)\n","\n","        return action.squeeze().cpu().numpy(), (obs_pred.squeeze().cpu().numpy().transpose(1, 2, 0) + 0.5).clip(0.0, 1.0)\n","\n","    #RNNの隠れ状態をリセット\n","    def reset(self):\n","        self.rnn_hidden = torch.zeros(1, self.rssm.rnn_hidden_dim, device=self.device)\n","\n","    def to(self, device):\n","        self.device = device\n","        self.encoder.to(device)\n","        self.decoder.to(device)\n","        self.rssm.to(device)\n","        self.action_model.to(device)\n","        self.rnn_hidden = self.rnn_hidden.to(device)\n","\n","\n","def preprocess_obs(obs):\n","    \"\"\"\n","    画像の変換. [0, 255] -> [-0.5, 0.5]\n","    \"\"\"\n","    obs = obs.astype(np.float32)\n","    normalized_obs = obs / 255.0 - 0.5\n","    return normalized_obs"]},{"cell_type":"markdown","metadata":{"id":"1nj9x0wgXWb3"},"source":["## 8. submission_tool の準備\n","- submission_tool_colab.zip をアップロードして解凍する．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0vNJTgU9Xjy4"},"outputs":[],"source":["import os\n","import zipfile\n","from pathlib import Path\n","\n","\n","# 解凍\n","with zipfile.ZipFile(\"submission_tool_ubuntu.zip\", \"r\") as zip_ref:\n","    zip_ref.extractall(\".\")"]},{"cell_type":"markdown","metadata":{"id":"9u8Q-aKfXYnT"},"source":["## 9. 提出物の作成\n","- 解凍した submission_tool の中身を利用して提出物を作成する．\n","- コマンドは基本修正しないでください．\n","  - エージェントのパラメータのパスのみ修正しても問題ございません．\n","- `import error` で student_code の import ができない場合， student_code.py が存在していない，もしくはファイル名が間違っている可能性があります．\n","- `Can't get attribute {class name}` のようにご自身で実装したクラス・関数がない場合もエラーが発生します．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Mrp8lTkYV-6"},"outputs":[],"source":["!python secure_submit.py --student student_code.py --model agent.pth --out submission.zip"]},{"cell_type":"markdown","metadata":{"id":"4gxvcOZIXa2Y"},"source":["## 10. 提出内容のプレビュー"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"12eyqtzIYa4f"},"outputs":[],"source":["# 作成された zip ファイルの構成の確認\n","if os.path.exists('submission.zip'):\n","    import zipfile\n","    with zipfile.ZipFile('submission.zip', 'r') as zip_file:\n","        for file_info in zip_file.filelist:\n","            file_size_kb = file_info.file_size / 1024\n","            print(f\"  📄 {file_info.filename:<12} ({file_size_kb:,.1f} KB)\")\n","\n","else:\n","    print(\"Submission file not found\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F1AbEzHOc3Sv"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","runtime_attributes":{"runtime_version":"2025.07"},"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"vscode":{"interpreter":{"hash":"1c906a337007ca492b40f9e66323e61f3dcaf71886120485625fb02da1be1aa9"}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}